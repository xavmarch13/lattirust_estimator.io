[{"id":0,"href":"/lattirust_estimator.io/docs/introduction/","title":"Introduction","section":"Docs","content":" Introduction # As quantum computing advances, many of the cryptographic techniques that underpin our current digital security may soon become vulnerable [1]. Quantum computers, leveraging algorithms like Shor’s [2], have the potential to break widely-used public key cryptosystems such as RSA and ECC, posing a serious threat to data confidentiality and integrity. In response, researchers are developing post-quantum cryptography (PQC)—new cryptographic algorithms designed to withstand quantum attacks [3].\nOne of the most promising approaches in PQC is lattice-based cryptography [4], which relies on the difficulty of certain mathematical problems involving high-dimensional lattices. Two of the most significant problems in this domain are the Short Integer Solution (SIS) problem [5] and the Learning With Errors (LWE) problem [6]. Both problems are related and widely believed to remain hard even in the post-quantum era, making them ideal candidates as the foundation for new cryptographic primitives.\nHowever, the effectiveness and practicality of lattice-based cryptography hinge on selecting the appropriate security parameters—such as the lattice dimension, modulus, and the error distribution (for LWE) [7]. These parameters directly influence the system’s resistance to quantum attacks, as well as its efficiency in real-world implementations. Striking the right balance is critical: overly conservative parameters may offer high security but at the cost of performance and practicality, while weaker parameters may leave the system vulnerable to attack. Moreover, the algorithms used to solve problems like SIS and LWE, known as lattice reduction algorithms [8], are still not fully understood, often performing better in practice than theoretical worst-case analysis would suggest.\nIn this blog, we will take a closer look at the SIS problem and its variants, and demonstrate the importance of security parameter estimation in building secure and efficient post-quantum cryptosystems. As part of our effort to port lattice-based systems from research to practice, we designed and implemented a Rust library for lattice parameter selection. Concretely, this library aims to:\nprovide an intuitive and hard-to-misuse interface for proof frontends, internally use precise estimates for the hardness of lattice problems (e.g., by reducing to the shortest vector problem), compute estimates for variants of standard problems of interest (e.g., new lattice assumptions, or standard assumptions with additional leakage). This library is to be integrated into lattirust, our library for lattice-based proofs. This work was conducted as part of a research project at the Computational Security Laboratory (COMPSEC) at EPFL.\nThe blog will be structured into several sections. First, we will explore the theoretical foundations of lattices and lattice reduction. Then, we will discuss the cost models that relate different methods of solving these problems to actual security estimates. After that, we will explain the SIS problem and its variants and how we model their security for the Euclidean ($L_2$) and infinity ($L_{\\infty}$) norms. Finally, we will provide a detailed walkthrough of how to use the Rust tool, including how to use it for security parameter estimation in practical applications.\nNote that our tool will be based on the state-of-the-art estimator developed by Martin Albrecht et al. [9]. Most of our design decisions will align with their work, as it is, to the best of our knowledge, the most precise and up-to-date estimator available for lattice-based problem security estimations. However, we will differ by centering our estimator on the SIS problem and its variants to specifically target zkSNARKs (zero-knowledge succinct non-interactive arguments). Additionally, we will port the estimator logic from Python and Sage into Rust, offering a modern and hopefully efficient implementation.\nReferences # A. Kitaev,\u0026#32;A. Shen, and\u0026#32;M. Vyalyi,\u0026#32;Classical and quantum computation.\u0026#32;American Mathematical Soc., 2002.\u0026#32;↩V. Bhatia and\u0026#32;K. Ramkumar,\u0026#32;An efficient quantum computing technique for cracking RSA using shor’s algorithm,\u0026#32;In Proc. 2020 IEEE 5th international conference on computing communication and automation (ICCCA),\u0026#32;2020, pp. 89–94.\u0026#32;↩D. Bernstein and\u0026#32;T. Lange,\u0026#32;Post-quantum cryptography,\u0026#32;Nature,\u0026#32;vol. 549,\u0026#32;no. 7671,\u0026#32;pp. 188–194,\u0026#32;2017.\u0026#32;↩D. Micciancio and\u0026#32;O. Regev,\u0026#32;Lattice-based cryptography,\u0026#32;in Post-quantum cryptography,\u0026#32;Springer, 2009, pp. 147–191.↩M. Ajtai,\u0026#32;Generating hard instances of lattice problems,\u0026#32;In Proc. Proceedings of the twenty-eighth annual ACM symposium on theory of computing,\u0026#32;1996, pp. 99–108.\u0026#32;↩O. Regev,\u0026#32;On lattices, learning with errors, random linear codes, and cryptography,\u0026#32;Journal of the ACM (JACM),\u0026#32;vol. 56,\u0026#32;no. 6,\u0026#32;pp. 1–40,\u0026#32;2009.\u0026#32;↩R. Player,\u0026#32;Parameter selection in lattice-based cryptography.\u0026#32;Royal Holloway, University of London, 2018.\u0026#32;↩P. Nguyen,\u0026#32;Lattice reduction algorithms: Theory and practice,\u0026#32;In Proc. Annual international conference on the theory and applications of cryptographic techniques,\u0026#32;2011, pp. 2–6.\u0026#32;↩M. Albrecht,\u0026#32;R. Player, and\u0026#32;S. Scott,\u0026#32;On the concrete hardness of learning with errors.\u0026#32;Cryptology ePrint Archive, Paper 2015/046, 2015.\u0026#32;[Online]. Available: https://eprint.iacr.org/2015/046\u0026#32; ↩ "},{"id":1,"href":"/lattirust_estimator.io/docs/preliminaries/","title":"Preliminaries","section":"Docs","content":" Preliminaries # This section is based on the following resources [1][2][3][4].\nLattices # A lattice is defined as a discrete subgroup in $n$-dimensional $\\mathbb{R}^n$ space with a periodic structure. It can be represented by a set of linearly independent vectors, commonly referred to as the basis of the lattice. If $\\bold{b_1}, \u0026hellip;, \\bold{b_d}$ denote basis vectors, we can describe a lattice as:\n$$ \\Lambda(\\bold{b_1}, ..., \\bold{b_d}) = \\left\\{\\sum_{i=1}^{d} x_i \\bold{b_i} : x_i \\in \\mathbb{Z} \\right\\} $$This represents the set of all linear combinations of the basis vectors. Here, $d$ is the dimension of the lattice in $\\mathbb{R}^n$.\nA lattice in 2D A lattice has many possible bases, but some are more useful than others. The goal of lattice reduction, as presented in later sections, is to find a qualitatively good basis composed of short and nearly orthogonal vectors.\nIn this work, we will specifically focus on $q$-ary lattices described by a basis $\\bold{B}$, where the coefficients are taken modulo $q$. The membership of a vector to the lattice is determined by $\\bold{x} \\pmod{q}$.\nVolume of a Lattice # In each lattice, we can define the volume of the lattice as the volume of its fundamental parallelepiped (the region enclosed by the basis vectors). This quantity is an invariant of the lattice and does not depend on the chosen basis. This means that applying Gram-Schmidt orthogonalization to any basis will yield an orthogonal basis from which we can approximate the volume of the lattice as:\n$$ \\text{Vol}(\\Lambda) = \\prod_{i=1}^{d} \\lVert \\bold{b_i}^* \\rVert $$where $\\bold{b_i}^*$ are the orthogonal Gram-Schmidt vectors. It is also important to note that $\\text{Vol}(\\Lambda) = |\\text{Det}(\\bold{B})|$, where $\\bold{B}$ is the basis matrix.\nA lattice volume in 2D This invariant is conceptually significant because it demonstrates that not all basis vectors can be small simultaneously.\nDual of a Lattice # The dual of a lattice $\\Lambda$ in $\\mathbb{R}^d$ is denoted $\\Lambda^*$ and is defined as the lattice composed of vectors $\\bold{y}$ such that:\n$$ \\langle \\bold{x}, \\bold{y} \\rangle \\in \\mathbb{Z}, \\forall \\bold{x} \\in \\Lambda $$For any lattice described by basis $\\bold{B}$, we have:\n$$ \\Lambda(\\bold{B})^* = \\Lambda((\\bold{B}^{-1})^T) \\quad \\text{and} \\quad \\text{Vol}(\\Lambda^*) = \\frac{1}{\\text{Vol}(\\Lambda)} $$ Minima of a Lattice and Root Hermite Factor # We denote by $\\lambda_i(\\bold{B})$ the $i$-th minimum of a lattice described by a basis $\\bold{B}$. Note that for the remainder of this work, we may interchangeably use $\\lambda_i(\\bold{B})$, $\\lambda_i(\\Lambda(\\bold{B}))$, or $\\lambda_i(\\Lambda)$ for minima and other related measures. Intuitively, $\\lambda_i(\\bold{B})$ represents the radius of the smallest zero-centered ball containing at least $i$ linearly independent lattice vectors.\nThe root Hermite constant $\\gamma_n$ is a constant that determines how short a lattice element can be. It is known exactly only for $n \\in {1, 2, 3, 4, 5, 6, 7, 8, 24}$, but upper bounds for other values are available and can be found in [5]. In general, determining $\\lambda_i(\\bold{B})$ is difficult, so we often refer to upper bounds.\nMinkowski\u0026rsquo;s Second Theorem: For any lattice in $\\mathbb{R}^n$ and any $1 \\leq d \\leq n$, we have:\n$$ \\left( \\prod_{i=1}^{d} \\lambda_i(\\Lambda) \\right)^{\\frac{1}{d}} \\leq \\sqrt{\\gamma_n} \\cdot \\text{Vol}(\\Lambda)^{\\frac{1}{n}} $$Formally, the Hermite constant for a full-rank lattice in $\\mathbb{R}^n$ is defined as:\n$$ \\gamma_n = \\sup_{\\Lambda \\subset \\mathbb{R}^n} \\frac{\\lambda_1^2(\\Lambda)}{\\det(\\Lambda)^{2/n}} $$The Hermite constant $\\gamma_n$ is closely related to the root Hermite factor $\\delta_n$, which we will describe in more detail in the lattice reduction chapter. The root Hermite factor is used to evaluate the quality of a basis, and we have $\\delta_n = \\sqrt{\\gamma_n}$.\nRandom Lattices and Gaussian Heuristic # The notion of a random lattice is mathematically complex (see Haar measures). As a simplifying assumption, imagine such a lattice being sampled from an existing random distribution that adheres, with overwhelming probability, to the Gaussian heuristic described below.\n$$ \\frac{\\lambda_i(\\Lambda)}{\\text{Vol}(\\Lambda)^{1/n}} \\approx \\frac{\\Gamma\\left(1 + \\frac{n}{2}\\right)^{1/n}}{\\sqrt{\\pi}} \\approx \\sqrt{\\frac{n}{2\\pi e}} $$The Gaussian heuristic predicts that the number of lattice points inside any measurable body $\\mathcal{B} \\subset \\mathbb{R}^d$ is approximately $\\frac{\\text{Vol}(\\mathcal{B})}{\\text{Vol}(\\Lambda)}$. Applied to a Euclidean $d$-ball, this implies that the length of the first vector is approximately:\n$$ \\lambda_1(\\Lambda) \\approx \\left(\\frac{\\text{Vol}(\\mathcal{B})}{\\text{Vol}(\\Lambda)}\\right)^{\\frac{1}{d}} \\approx \\sqrt{\\frac{d}{2\\pi e}} \\cdot \\text{Vol}(\\Lambda)^{\\frac{1}{d}} $$ Hard Problems in Lattices # The security of lattice-based constructions relies on several variants of related problems. The foundation of this security is based on the fact that finding a short non-zero vector in a lattice is computationally hard. Below are the definitions of the key problems:\nShortest Vector Problem (SVP)\nDefinition:\nGiven a lattice $\\Lambda$ with basis $\\bold{B}$, the Shortest Vector Problem (SVP) is the problem of finding a non-zero vector $\\bold{v} \\in \\Lambda$ such that:\n$$ \\|\\bold{v}\\| = \\lambda_1(\\Lambda) $$where $\\lambda_1(\\Lambda)$ is the length of the shortest non-zero vector in the lattice. This is a well-known NP-hard problem in the worst case, and its approximation remains challenging for cryptographic settings.\nHermite Shortest Vector Problem (H-SVP)\nDefinition:\nThe Hermite Shortest Vector Problem (H-SVP) is a scaled version of the SVP. It asks for a non-zero vector $\\bold{v} \\in \\Lambda$ such that:\n$$ \\|\\bold{v}\\| \\leq \\delta_n \\cdot \\text{Vol}(\\Lambda)^{1/n} $$where $\\delta_n$ is the root Hermite factor. Unlike exact SVP, H-SVP allows finding a vector that is \u0026ldquo;short enough\u0026rdquo; relative to the lattice volume. H-SVP is easier to approximate than SVP and is central to lattice reduction algorithms like LLL and BKZ.\nApproximate Shortest Vector Problem (Approx-SVP)\nDefinition:\nThe Approximate Shortest Vector Problem (Approx-SVP) generalizes SVP by relaxing the requirement to find the exact shortest vector. Given a lattice $\\Lambda$ and an approximation factor $\\gamma \\geq 1$, the goal is to find a non-zero vector $\\bold{v} \\in \\Lambda$ such that:\n$$ \\|\\bold{v}\\| \\leq \\gamma \\cdot \\lambda_1(\\Lambda) $$For sufficiently large $\\gamma$, Approx-SVP becomes computationally easier, but it remains hard for small $\\gamma$, especially in high-dimensional lattices.\nUnique Shortest Vector Problem (Unique-SVP)\nDefinition:\nThe Unique Shortest Vector Problem (Unique-SVP) is a variant of SVP where the shortest vector is guaranteed to be significantly shorter than all other lattice vectors. Specifically, there exists a gap $\\beta \u0026gt; 1$ such that:\n$$ \\lambda_2(\\Lambda) \\geq \\beta \\cdot \\lambda_1(\\Lambda) $$where $\\lambda_2(\\Lambda)$ is the length of the second shortest lattice vector. The task is to find the unique shortest vector $\\bold{v} \\in \\Lambda$. This problem is somewhat easier than general SVP due to the presence of a unique solution, but it still remains computationally challenging.\nClosest Vector Problem (CVP)\nDefinition:\nGiven a lattice $\\Lambda$ with basis $\\bold{B}$ and a target vector $\\bold{t} \\in \\mathbb{R}^n$, the Closest Vector Problem (CVP) is the problem of finding a vector $\\bold{v} \\in \\Lambda$ such that:\n$$ \\|\\bold{t} - \\bold{v}\\| = \\min_{\\bold{w} \\in \\Lambda} \\|\\bold{t} - \\bold{w}\\| $$CVP is generally harder than SVP and is also NP-hard in the worst case. Approximation versions of CVP, such as $\\gamma$-CVP (finding $\\bold{v}$ within a $\\gamma$-factor of the closest vector), are frequently studied in lattice cryptography.\nShortest Independent Vector Problem (SIVP)\nDefinition:\nThe Shortest Independent Vector Problem (SIVP) involves finding $n$ linearly independent vectors ${\\bold{v}_1, \\bold{v}_2, \\dots, \\bold{v}_n}$ in a lattice $\\Lambda$ such that the maximum norm of the vectors is minimized:\n$$ \\max_{i=1}^n \\|\\bold{v}_i\\| = \\lambda_n(\\Lambda) $$where $\\lambda_n(\\Lambda)$ represents the length of the $n$-th successive minimum of the lattice. SIVP is closely related to the geometry of the lattice and remains computationally hard. It is used in cryptographic constructions and reduction proofs, often connecting the hardness of lattice problems to cryptographic assumptions.\nSummary Table of Lattice Problems # Problem Goal Output Difficulty (I)SVP Find the shortest vector $\\bold{v} \\in \\Lambda, |\\bold{v}| = \\lambda_1$ NP-hard in the worst case H-SVP Find a short enough vector $\\bold{v} \\in \\Lambda, |\\bold{v}| \\leq \\delta_n \\cdot \\text{Vol}(\\Lambda)^{1/n}$ Easier than SVP, used in reduction Approx-(I)SVP Approximate the shortest vector $\\bold{v} \\in \\Lambda, |\\bold{v}| \\leq \\gamma \\lambda_1$ Hard for small $\\gamma$ Unique-(I)SVP Find the unique shortest vector $\\bold{v} \\in \\Lambda, \\lambda_2 \\geq \\beta \\lambda_1$ Easier than SVP (gap $\\beta$) CVP Find the closest lattice vector to target $\\bold{v} \\in \\Lambda, |\\bold{t} - \\bold{v}|$ NP-hard, harder than SVP Importantly, for later sections, note that any algorithm solving Approx-SVP with factor $\\alpha$ also solves Hermite-SVP for factor $\\alpha\\sqrt{\\gamma_n}$ in polynomial time. Additionally, any algorithm solving H-SVP with factor $\\alpha$ can be used a linear number of times to solve Approx-SVP with factor $\\alpha^2$ in polynomial time [6]. There also exist reductions from worst-case Approx-SVP to average-case H-SVP for certain classes of lattices.\nSecurity # The foundational belief in the security of new lattice-based primitives stems from a key result: Ajtai’s theorem, which connected the hardness of certain average-case problems to the difficulty of worst-case problems in lattices [7]. Specifically, Ajtai demonstrated that for the Short Integer Solution (SIS) problem—which we will define later—the average-case instances are at least as hard as the worst-case instances of the Shortest Vector Problem (SVP) on lattices. This implies that if one could efficiently solve random instances of SIS, they could also solve the worst-case SVP, a problem believed to be intractable even for quantum computers. Ajtai’s theorem provides a strong security guarantee for lattice-based cryptographic schemes by grounding their security in the hardness of well-studied lattice problems like SVP.\nThe security assumptions based on the hardness of SVP can be summarized by two conjectures from [3]:\nThere is no polynomial-time algorithm that approximates lattice problems to within polynomial factors. There is no polynomial-time quantum algorithm that approximates lattice problems within polynomial factors. These conjectures underpin the belief that lattice-based cryptosystems remain secure even in the presence of quantum adversaries, making them promising candidates for post-quantum security.\nNorms # We provide a brief overview of norms that will be used later and their relationships. We also introduce norms for elements in modules and rings.\nDefinition of \\( \\ell_p \\)-Norms # For a vector $\\boldsymbol{f} \\in \\mathbb{R}^n$, the $\\ell_p$-norms are defined as follows:\n$$ \\|\\boldsymbol{f}\\|_1 = \\sum_{i=1}^n |f_i|, \\quad \\|\\boldsymbol{f}\\|_2 = \\sqrt{\\sum_{i=1}^n f_i^2}, \\quad \\|\\boldsymbol{f}\\|_p = \\left(\\sum_{i=1}^n |f_i|^p\\right)^{1/p}, \\quad \\|\\boldsymbol{f}\\|_\\infty = \\max_{i} |f_i|. $$These norms generalize to module elements $\\boldsymbol{f} \\in R_q^m$ (where $R_q$ is a quotient ring, e.g., $\\mathbb{Z}_q[x]/\\langle x^n + 1 \\rangle$), by viewing them as $m \\cdot n$-dimensional vectors [8]. Note that modules generalize rings, so a vector $\\boldsymbol{f}$ in the ring $R_q^1$ is represented by a set of coefficients $f_i$ such that:\n$$ \\boldsymbol{f} = \\sum_i f_i x^i. $$ Relationships Between Norms # For $p, q \\geq 1$ with $p \\leq q$, the following relationships hold for $\\boldsymbol{f} \\in \\mathbb{R}^n$:\n$$ \\|\\boldsymbol{f}\\|_\\infty \\leq \\|\\boldsymbol{f}\\|_q \\leq \\|\\boldsymbol{f}\\|_p \\leq \\|\\boldsymbol{f}\\|_1. $$This inequality indicates that the $\\ell_\\infty$-norm is always the smallest, while the $\\ell_1$-norm is the largest.\nApplications in Lattice Problem Hardness Estimation # Norms are crucial for bounding errors and ensuring security in lattice-based cryptography. For example:\nIn the Learning with Errors (LWE) problem, the $\\ell_2$-norm is used to bound the error vector. In the Short Integer Solution (SIS) problem, the $\\ell_\\infty$-norm is often employed to restrict the coefficients of the solution. By carefully analyzing and relating these norms, cryptographic schemes ensure robustness against attacks while maintaining efficiency.\nReferences # N. Gama and\u0026#32;P. Nguyen,\u0026#32;Predicting lattice reduction,\u0026#32;In Proc. Advances in cryptology–EUROCRYPT 2008: 27th annual international conference on the theory and applications of cryptographic techniques, istanbul, turkey, april 13-17, 2008. Proceedings 27,\u0026#32;2008, pp. 31–51.\u0026#32;↩D. Micciancio and\u0026#32;S. Goldwasser,\u0026#32;Complexity of lattice problems: A cryptographic perspective.\u0026#32;Springer Science \u0026amp; Business Media, 2002.\u0026#32;↩D. Micciancio and\u0026#32;O. Regev,\u0026#32;Lattice-based cryptography,\u0026#32;in Post-quantum cryptography,\u0026#32;Springer, 2009, pp. 147–191.↩P. Nguyen,\u0026#32;Hermite’s constant and lattice algorithms,\u0026#32;in The LLL algorithm: Survey and applications,\u0026#32;Springer, 2009, pp. 19–69.↩H. Cohn and\u0026#32;N. Elkies,\u0026#32;New upper bounds on sphere packings i,\u0026#32;Annals of mathematics,\u0026#32;pp. 689–714,\u0026#32;2003.\u0026#32;↩L. Lovász,\u0026#32;An algorithmic theory of numbers, graphs and convexity.\u0026#32;SIAM, 1986.\u0026#32;↩M. Ajtai,\u0026#32;Generating hard instances of lattice problems,\u0026#32;In Proc. Proceedings of the twenty-eighth annual ACM symposium on theory of computing,\u0026#32;1996, pp. 99–108.\u0026#32;↩C. Baum,\u0026#32;I. Damgård,\u0026#32;V. Lyubashevsky,\u0026#32;S. Oechsner, and\u0026#32;C. Peikert,\u0026#32;More efficient commitments from structured lattice assumptions,\u0026#32;In Proc. International conference on security and cryptography for networks,\u0026#32;2018, pp. 368–385.\u0026#32;↩ "},{"id":2,"href":"/lattirust_estimator.io/docs/cost-models/","title":"Cost Models","section":"Docs","content":" Cost Models # Given the simplified cost for BKZ behaviour that we consider $cost = \\tau \\cdot d \\cdot T_{SVP}$, we still need to define the cost of the SVP solver. Sieving and enumeration are the two common exact strategies to find the shortest non-zero vector in a lattice.\nSieving # Sieving algorithms for SVP work by iteratively refining a large set of lattice vectors to obtain progressively shorter vectors until the shortest one is found. In its basic form, the sieving process starts by generating a large set of random lattice vectors, often called a \u0026ldquo;cloud.\u0026rdquo; Pairs of vectors are then combined (usually by subtracting them) to produce shorter vectors, which are then added back to the cloud if they meet certain criteria. This process continues until the vectors in the cloud converge towards the shortest lattice vector. Modern sieving methods, like the GaussSieve or ListSieve, have been optimized to handle higher-dimensional lattices by limiting pairwise vector interactions, which reduces computational complexity. Sieving requires storing a large number of lattice vectors and, therefore, can be memory intensive, especially as the lattice dimension grows. Here we present a simplified graphs of selecting two vectors out of a cloud, taking the difference between two vectors and looking what is the smaller between the three vectors.\nSmall example of the sieving logic. In terms of cost, sieving algorithm can solve the SVP in a lattice of dimension d in $2^{O(\\beta)}$ time but at the cost of a much higher memory usage $2^{O(\\beta)}$ than enumeration. Our estimator will disregard memory usage to focus on time costs.\nThe following sieving estimates are available in the estimator:\nName Reference Cost Regime BDGL-sieve [1] $2^{0.292\\beta + 16.4}$ big $\\beta$ $2^{0.387\\beta + 16.4}$ small $\\beta$ classical Q-sieve [2] [3] $2^{0.265\\beta}$ quantum ADPS-sieve [2] $2^{0.292\\beta}$ classical BGJ-sieve [4] $2^{0.311\\beta}$ classical ChaLoy-sieve [5] $2^{0.257\\beta}$ quantum Kyber-sieve [6] $5.46 \\times 2^{0.2988\\beta + 26.0111}$ classical Kyber-sieve [6] $5.46 \\times 2^{0.2694\\beta + 28.9724}$ quantum Matzov-sieve [7] $5.46 \\times 2^{0.2961\\beta + 20.3879}$ classical Matzov-sieve [7] $5.46 \\times 2^{0.2664\\beta + 25.2995}$ quantum Cost of sieving SVP solvers As a very high overview, we give a list of the amelioration brought by each kind of sieving solver considered :\nBDGL-sieve introduces a novel approach to improving lattice sieving efficiency through locality-sensitive filtering (LSF), which significantly reduces the time complexity of solving the shortest vector problem (SVP) on high-dimensional lattices. By utilizing spherical caps as filters and leveraging structured random product codes for efficient decoding, the method achieves an asymptotic time complexity of \\(2^{0.292n + o(n)}\\), outperforming previous approaches like spherical locality-sensitive hashing (LSH).\nADPS-sieve presents sintroduces quantum speedups through Grover’s algorithm, reducing the classical sieving complexity from \\(2^{0.415b}\\) to \\(2^{0.292b}\\). Additionally, it leverages structured lattice representations to optimize memory usage and sieve iterations. This method focuses on balancing theoretical improvements with practical implementations, providing a framework for faster and more scalable sieving for lattice-based cryptographic challenges.\nBGJ-sieve significantly reduces the time complexity to \\(2^{0.292d + o(d)}\\) through algorithmic optimizations. Key contributions include the introduction of hypersphere partitioning to reduce vector comparison costs, the use of nearby vector replacement to iteratively refine the list of vectors towards shorter ones, and a probabilistic framework that efficiently handles high-dimensional spaces.\nChaLoy-sieve introduces a quantum algorithm based on quantum random walks that improves the asymptotic complexity of solving the Shortest Vector Problem (SVP). The key innovation is replacing Grover\u0026rsquo;s algorithm with a quantum random walk, enabling faster resolution of reducible vector pairs. This enhancement reduces the heuristic running time from \\(2^{0.2653d + o(d)}\\) to \\(2^{0.2570d + o(d)}\\), while optimizing resource usage. Notably, it decreases quantum memory requirements to \\(2^{0.0495d}\\) and quantum RAM to \\(2^{0.0767d}\\).\nKyber-sieve leverages the hardness of the Module Learning With Errors (MLWE) problem in module lattices, making it highly efficient due to its use of number-theoretic transforms (NTT) and structured lattice encodings. The Kyber cryptosystem improves lattice reductions by balancing the complexity of sieving-based attacks through modular parameter tuning, including optimized noise levels for secure encryption.\nMatzov-sieve presents significant improvements to dual lattice attacks by introducing optimizations for the cost of sieving in the RAM model, improving the efficiency of decoding using random product codes, and reducing the number of gates required for each operation. The attack leverages a tailored FFT-based distinguisher combined with modulus switching to reduce the overall computation while maintaining accuracy. Additionally, the approach applies advanced parameter tuning to optimize the BKZ-based reduction and introduces techniques such as \u0026ldquo;dimensions-for-free\u0026rdquo; to further reduce the dimension of the lattice needed for effective cryptanalysis.\nThe Dimension for free # In recent advancements, Ducas introduces a significant improvement in lattice sieving algorithms through the \u0026ldquo;dimension for free\u0026rdquo; technique [8]. This approach leverages the observation that short lattice vectors generated during sieving in lower dimensions can be reused to accelerate (solve) sieving in higher dimensions. By projecting and reusing these vectors across multiple dimensions, the algorithm effectively reduces the computational overhead without compromising accuracy. This method achieves a practical speedup of up to 10x in mid-range dimensions (e.g., 70–80). Our estimator will automatically provide this improvement by adapting the size of the needed lattice reduction computation when estimating with the infinity norm. When we use the Euclidean norm, this speed-up is less important since we have an equation from block-size to quality. In practice, we can reduce the SVP problem of dimension $n$ to solving an SVP problem of dimension $n-d$ as long as $d$ is $\\Theta(\\frac{n}{\\log(n)})$. The paper mentions a concrete prediction of $d \\approx \\frac{n\\ln(4/3)}{\\ln(n/2\\pi e)}$.\nEnumeration # Enumeration algorithms systematically search through lattice points in a controlled way, typically by traversing lattice vectors within a fixed radius from the origin. They rely on a recursive process to explore potential candidate vectors within a \u0026ldquo;search region,\u0026rdquo; using techniques to prune paths that are unlikely to lead to the shortest vector. Enumeration is typically carried out with the help of a basis that has been reduced (made close to orthogonal), as this greatly improves efficiency by minimizing the number of candidate paths. Unlike sieving, enumeration methods are deterministic and guarantee finding the shortest vector by systematically exploring all feasible paths. The efficiency of enumeration depends strongly on the quality of the lattice basis. Preprocessing steps like BKZ (Block Korkine-Zolotarev) reduction can make enumeration significantly faster by transforming the basis to be more suitable for search. Enumeration is often practical for lower-dimensional lattices or when a high degree of accuracy is needed, but it tends to be less efficient than sieving in high dimensions due to its exponential complexity. Hereafter we present a simplified version of how enumeration could progress. Given a basis b1 and b2, we project one of the two and use it to make a grid spaced by the length of the projection. Using this 1-dim problem we can lift back up to the 2-d problem by selecting all points in the radius that are on the grid.\nSmall example of the enumeration logic. In terms of cost, enumeration can solve SVP in a lattice of dimension d in $2^{O(\\beta \\log \\beta)}$ time and space poly($\\beta$).\nThe following enumeration estimates are available:\nName Reference Cost Regime Lotus [4] [9] $2^{0.125\\beta\\log\\beta -0.755\\beta + 22.74}$ classical CheNgue-enum (BKZ 2.0) [10] $2^{0.27\\beta\\log\\beta -1.019\\beta + 2.254}$ classical ABF-enum [11] $2^{0.184\\beta\\log\\beta - 0.995\\beta + 22.25}$ small $\\beta$ $2^{0.125\\beta\\log\\beta-0.547\\beta+16.4}$ big $\\beta$ classical ABF-Q-enum [11] $2^{0.0625\\beta\\log\\beta}$ quantum ABLR-enum [12] $2^{0.184\\beta\\log\\beta - 1.077\\beta + 35.12}$ small $\\beta$ $2^{0.125\\beta\\log\\beta-0.655\\beta+31.84}$ big $\\beta$ classical Here are some plots to better visualize enumeration costs.\nCost of enumeration SVP solvers As a very high overview, we give a list of amelioration brought by each kind of enumeration solver considered :\nABF-Enum introduces the concept of extended preprocessing. This approach decouples the preprocessing and enumeration contexts, enabling the use of higher-dimensional sublattices for preprocessing while optimizing the enumeration cost. By leveraging the enhanced geometric properties of SDBKZ-reduced bases, ABF-Enum achieves faster enumeration with reduced computational complexity, pushing the boundaries of lattice reduction efficiency.\nBKZ 2.0 (ChNgue) optimizes the Blockwise Korkine-Zolotarev (BKZ) algorithm. Key enhancements include the incorporation of sound pruning, a sophisticated technique to reduce enumeration cost while maintaining quality, and preprocessing of local blocks to improve the efficiency of enumeration subroutines. Additionally, optimized enumeration radii enable faster convergence by leveraging improved heuristics. These innovations collectively improve both the speed and output quality of lattice reduction, making BKZ 2.0 a new benchmark for practical lattice algorithms.\nABLR-Enum innovates by combining relaxed pruning with extended preprocessing strategies. These enhancements allow enumeration algorithms to achieve exponential speed-ups while maintaining high-quality reductions. By relaxing the search radius and optimizing enumeration parameters, the approach efficiently balances time and output quality.\nThe LOTUS cryptosystem enhances lattice enumeration techniques through optimized pruning strategies, which reduce the computational effort required for solving lattice problems like bounded distance decoding (BDD).\nMethod comparison # Cost of all solvers We can see that sieving performs better overall when the dimension of the underlying SVP problem get big, which is the expected behavior. For this reason, we encourage making security estimates with sieving as an underlying SVP solver. The security estimate we will use as defaults will be the Matzov estimate.\nReferences # A. Becker,\u0026#32;L. Ducas,\u0026#32;N. Gama, and\u0026#32;T. Laarhoven,\u0026#32;New directions in nearest neighbor searching with applications to lattice sieving,\u0026#32;In Proc. Proceedings of the twenty-seventh annual acm-siam symposium on discrete algorithms,\u0026#32;2016, pp. 10–24.\u0026#32;↩E. Alkim,\u0026#32;L. Ducas,\u0026#32;T. Pöppelmann, and\u0026#32;P. Schwabe,\u0026#32;Post-quantum key Exchange—A new hope,\u0026#32;In Proc. 25th usenix security symposium (usenix security 16),\u0026#32;2016, pp. 327–343.\u0026#32;↩T. Laarhoven,\u0026#32;M. Mosca, and\u0026#32;J. Van De Pol,\u0026#32;Finding shortest lattice vectors faster using quantum search,\u0026#32;Designs, Codes and Cryptography,\u0026#32;vol. 77,\u0026#32;pp. 375–400,\u0026#32;2015.\u0026#32;↩M. Albrecht,\u0026#32;B. Curtis,\u0026#32;A. Deo,\u0026#32;A. Davidson,\u0026#32;R. Player,\u0026#32;E. Postlethwaite,\u0026#32;F. Virdia, and\u0026#32;T. Wunderer,\u0026#32;Estimate all the ${$lwe, ntru$}$ schemes!,\u0026#32;In Proc. Security and cryptography for networks: 11th international conference, scn 2018, amalfi, italy, september 5–7, 2018, proceedings 11,\u0026#32;2018, pp. 351–367.\u0026#32;↩A. Chailloux and\u0026#32;J. Loyer,\u0026#32;Lattice sieving via quantum random walks,\u0026#32;In Proc. Advances in cryptology–asiacrypt 2021: 27th international conference on the theory and application of cryptology and information security, singapore, december 6–10, 2021, proceedings, part iv 27,\u0026#32;2021, pp. 63–91.\u0026#32;↩V. Lyubashevsky,\u0026#32;L. Ducas,\u0026#32;E. Kiltz,\u0026#32;T. Lepoint,\u0026#32;P. Schwabe,\u0026#32;G. Seiler,\u0026#32;D. Stehlé, and\u0026#32;S. Bai,\u0026#32;Crystals-dilithium,\u0026#32;Algorithm Specifications and Supporting Documentation,\u0026#32;2020.\u0026#32;↩MATZOV,\u0026#32;Report on the security of lwe: Improved dual lattice attack.\u0026#32;Zenodo, 2022.\u0026#32;doi:10.5281/zenodo.6493704↩L. Ducas,\u0026#32;Shortest vector from lattice sieving: A few dimensions for free,\u0026#32;In Proc. Annual international conference on the theory and applications of cryptographic techniques,\u0026#32;2018, pp. 125–145.\u0026#32;↩T. Le Trieu Phong,\u0026#32;Y. Aono, and\u0026#32;S. Moriai,\u0026#32;Lotus.\u0026#32;Technical report, National Institute of Standards; Technology, 2017 …, 2017.\u0026#32;↩Y. Chen and\u0026#32;P. Nguyen,\u0026#32;BKZ 2.0: Better lattice security estimates,\u0026#32;In Proc. International conference on the theory and application of cryptology and information security,\u0026#32;2011, pp. 1–20.\u0026#32;↩M. Albrecht,\u0026#32;S. Bai,\u0026#32;P. Fouque,\u0026#32;P. Kirchner,\u0026#32;D. Stehlé, and\u0026#32;W. Wen,\u0026#32;Faster enumeration-based lattice reduction: Root hermite factor time,\u0026#32;In Proc. Annual international cryptology conference,\u0026#32;2020, pp. 186–212.\u0026#32;↩M. Albrecht,\u0026#32;S. Bai,\u0026#32;J. Li, and\u0026#32;J. Rowell,\u0026#32;Lattice reduction with approximate enumeration oracles: Practical algorithms and concrete performance,\u0026#32;In Proc. Annual international cryptology conference,\u0026#32;2021, pp. 732–759.\u0026#32;↩ "},{"id":3,"href":"/lattirust_estimator.io/docs/lattice-reduction/","title":"Lattice reduction","section":"Docs","content":" How to solve SVP ? # The way we solve the Shortest Vector Problem (SVP) and similar problems depends significantly on the lattice dimension. In lower dimensions, exact solvers are practical, and there are two main approaches: enumeration and sieving. Both methods perform an exhaustive search over all short lattice vectors—enumeration does this deterministically, while sieving is typically randomized. However, as the lattice dimension grows, the number of possible solutions increases exponentially, making these methods infeasible for high dimensions (think 100 and more).\nIn higher dimensions, we rely on approximation algorithms, better known as lattice reduction algorithms. These algorithms don’t find the exact solution but instead provide an approximation where the vector length is upper bounded by a function of the dimension. Lattice reduction can be thought of as the algorithmic equivalent of inequalities like Hermite’s and Mordell’s [1], which bound the shortest vector length in theoretical terms.\nHermite\u0026rsquo;s inequality: $$\\forall d \\geq 2: \\gamma_d \\leq \\left(\\sqrt{\\frac{4}{3}}\\right)^{d-1}$$ Mordell\u0026rsquo;s inequality: $$\\forall d,k \\text{ such that } 2\\leq k \\leq d: \\gamma_d \\leq \\sqrt{\\gamma_k}^{\\frac{d-1}{k-1}}$$ In practice, both exact and approximate solvers are used together. Exact solvers usually start with a preprocessing step using lattice reduction to simplify the problem. On the other hand, lattice reduction algorithms often call exact solvers as subroutines, using them multiple times during their process. This section focuses on explaining the cost of approximation algorithms such as LLL and BKZ as a whole, and we will refer to the cost of the exact SVP solver used as a subroutine as the \u0026ldquo;cost models,\u0026rdquo; described in the next section.\nLattice reduction # Lattice reduction algorithms are designed to transform a given basis of a lattice into a \u0026ldquo;reduced\u0026rdquo; basis, where the vectors are shorter and closer to being orthogonal. While a lattice does not generally have an orthogonal basis (unlike in Euclidean space), the goal of lattice reduction is to produce a basis that approximates orthogonality as closely as possible. This transformation simplifies solving challenging lattice problems such as the Shortest Vector Problem (SVP).\nThis section is based on the works of [1], [2], [3], [4], [5], [6], and [7].\nTwo different bases, one being close to orthogonal. Key Lattice Reduction Algorithms # LLL Algorithm (Lenstra–Lenstra–Lovász): This algorithm produces a reduced basis in polynomial time. The resulting basis vectors are guaranteed to be within a known factor of the shortest vector, although the algorithm does not necessarily find the shortest vector itself [5]. BKZ Algorithm (Block Korkine-Zolotarev): A more advanced generalization of the LLL algorithm that provides stronger reductions at the cost of increased computational complexity. The BKZ algorithm divides the lattice into overlapping blocks and applies LLL reduction within each block, achieving more accurate approximations of the shortest vector [4]. Cost Considerations # Both LLL and BKZ make iterative local improvements to a basis by using an exact SVP solver (often referred to as an SVP oracle). Therefore, the overall cost of the algorithm can be divided into two components:\nLocal cost: The computational cost associated with solving the SVP within each block. Global cost: The number of times the algorithm needs to invoke the SVP oracle during the entire basis reduction process. Together, these components determine the total cost of performing lattice reduction.\nGram-Schmidt Orthogonalization # Gram-Schmidt orthogonalization is a method for orthonormalizing a set of vectors in an inner product space, most commonly the Euclidean space $\\mathbb{R}^n$. The process transforms a set of linearly independent vectors into an orthogonal set of vectors that spans the same subspace.\nGiven a set of linearly independent vectors ${\\mathbf{b}_1, \\mathbf{b}_2, \\ldots, \\mathbf{b}_n}$, the Gram-Schmidt process produces an orthogonal set ${\\mathbf{b}^{*}_1, \\ldots, \\mathbf{b}^*_n}$ as follows:\n$\\mathbf{b}_1^* = \\mathbf{b}_1$ For $i = 2$ to $n$: $$ \\mathbf{b}_i^* = \\mathbf{b}_i - \\sum_{j=1}^{i-1} \\text{proj}_{\\mathbf{b}_j^*}(\\mathbf{b}_i) $$ where $\\text{proj}_{\\mathbf{b}_j^{*}}(\\mathbf{b}_i)$ is the projection of $\\mathbf{b}_i$ onto $\\mathbf{b}_j^*$, given by: $$ \\text{proj}_{\\mathbf{b}_j^*}(\\mathbf{b}_i) = \\frac{\\langle \\mathbf{b}_i, \\mathbf{b}_j^* \\rangle}{\\langle \\mathbf{b}_j^*, \\mathbf{b}_j^* \\rangle} \\mathbf{b}_j^*. $$ Gram-Schmidt orthogonalization is widely used in lattice reduction because it allows the basis to be triangularized. More precisely, it transforms the basis into the following form:\n$$ \\begin{pmatrix} \\|\\mathbf{b}_1^*\\| \u0026 0 \u0026 \\ldots \u0026 0 \\\\ \\mu_{2,1} \\|\\mathbf{b}_1^*\\| \u0026 \\|\\mathbf{b}_2^*\\| \u0026 \\ldots \u0026 0 \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\mu_{d,1} \\|\\mathbf{b}_1^*\\| \u0026 \\mu_{d,2} \\|\\mathbf{b}_2^*\\| \u0026 \\ldots \u0026 \\|\\mathbf{b}_d^*\\| \\end{pmatrix} $$Thus, $B = \\mu B^{*}$ [7][1]. We can confirm from this matrix that $\\text{Vol}(\\Lambda) = \\prod_{i=1}^{d} \\lVert \\bold{b}_i^* \\rVert$ (the determinant is the product of the diagonal elements).\nWe can also state the following lemma that relates the shortest vector to the Gram-Schmidt vectors for all $1 \\leq i \\leq d$:\n$$ \\lambda_i(\\Lambda) \\geq \\min_{i \\leq j \\leq d} \\|\\mathbf{b}_j^*\\|. $$Remember that the volume is an invariant, so not all Gram-Schmidt vectors can be small at the same time.\nRoot hermite factor # We will next want to introduce a value called the root hermite factor. It is a measure used in lattice reduction theory to evaluate the quality of a reduced lattice basis. It is commonly used to assess the effectiveness of lattice reduction algorithms.\n$$ \\delta = \\left( \\frac{\\|\\mathbf{b}_1\\|}{\\text{vol}(\\Lambda)^{1/d}} \\right)^{1/d} $$ for a d-dimensional lattice.\nThe closer $\\delta$ gets to 1, the better the reduction quality will be. This is of direct impact in our context, since we need to balance a trade-off between the quality of the output basis and the cost of running our lattice reduction algorithm. In fact, with the BKZ algorithm which has become the standard, a bigger block-size leads to a better quality of output basis (so a better $\\delta_\\beta$) but also a greater computational cost.\nGeometric Series Assumption (GSA) # How large the minimas can be after lattice reduction is therefore looking for how short we expect the basis vectors to be after applying lattice reduction (which contains Gram-Schmidt orthogonalization). It is often useful to look at the length of all gram-schmidt vectors, not only the first one. As a small experiment, let us consider a basis in $\\mathbb{Z}_q$ and let us compute the Gram-Schmidt orthogonalization. If we look at the log of these lengths we obtain the Z-shape because the first are orthogonal components of q magnitude and the rest are all vectors of length 1:\nLength of log GS vectors If we now apply a lattice reduction algorithm (here LLL), we will obtain this:\nLength of vectors after LLL You can observe this merely looks like a straight line and indeed this is the assumption that we will make. The Geometric Series Assumption conceptually tells us that the Gram-Schmidt vectors log-length outputed by a lattice reducion algorithm will follow a geometric series and such a line in log lengths. We can formulate it as:\n$$\\lVert \\bold{b_i}^*\\rVert \\approx \\alpha^{i-1}\\lVert \\bold{b_1}\\rVert$$and in fact by combining it with the fact that output vector of lattice reduction follow $\\lVert \\bold{b_0} \\rVert = \\delta_0 Vol(\\Lambda)^{\\frac{1}{d}}$ we can get a relation between the quality of the reduction and the slope of the GSA assumption as $\\alpha \\approx \\delta^{-2}$ leading to\n$$ \\lVert \\bold{b_i}^*\\rVert \\approx \\alpha^{i-1} \\delta^d Vol(\\Lambda)^\\frac{1}{d} = \\delta^{-1(i+1) + d} Vol(\\Lambda)^\\frac{1}{d}$$ LLL algorithm # The Lenstra-Lenstra-Lovász (LLL) algorithm is an efficient polynomial-time algorithm that finds a \u0026ldquo;nearly orthogonal\u0026rdquo; basis for a given lattice. It aims to transform any arbitrary basis of a lattice into a reduced basis where the basis vectors are short and close to orthogonal following two conditions:\nSize reduction: $$1 \\leq j \u003c i \\leq d\\colon \\left|\\mu_{i,j}\\right|\\leq 0.5 \\text{ for } \\mu_{i,j} =\\frac{\\langle\\mathbf{b}_i,\\mathbf{b}^*_j\\rangle}{\\langle\\mathbf{b}^*_j,\\mathbf{b}^*_j\\rangle}$$ Lovász condition: For $k=2,\u0026hellip;,d$ $$\\omega \\Vert \\mathbf{b}^*_{k-1}\\Vert^2 \\leq \\Vert \\mathbf{b}^*_k\\Vert^2+ \\mu_{k,k-1}^2\\Vert\\mathbf{b}^*_{k-1}\\Vert^2$$ We say the basis is LLL-reduced if there exists a parameter $\\omega \\in (0.25, 1)$. Hereafter we give a pseudo code of the algorithm and a graphical example of its run.\nData: a basis B Repeat until no changes: for k = 0 to d-1: # Step 1: Gram-Schmidt Orthogonalization (GSO) for i = k+1 to d-1: mu[k, i] = \u0026lt;B[k], B[i]\u0026gt; / \u0026lt;B[k], B[k]\u0026gt; B[i] = B[i] - mu[k, i] * B[k] # Step 2: Size Reduction for i = k+1 to d-1: if |mu[k, i]| \u0026gt;= 1/2: v = round(mu[k, i]) B[i] = B[i] - v * B[k] # Step 3: Swap if necessary if \u0026lt;B[k+1], B[k+1]\u0026gt; \u0026lt; (delta * \u0026lt;B[k], B[k]\u0026gt;): Swap B[k], B[k+1] end Example of the LLL algorithm running. Although there are theorems bounding the worst-case performance of lattice reduction algorithms, they tend to perform better in practice. Reasoning about the behavior of such algorithms has therefore become a matter of heuristics and approximations. Typically, the vectors that are output by the LLL algorithm are said to follow the geometric series assumption in their length. Again, this assumption tells us that the shape after lattice reduction forms a line with a flatter slope as the reduction becomes stronger. The goal of a lattice reduction algorithm can therefore be visualized by examining a plot of the log-length of vectors after reductions. The overall goal is to flatten this line, leading to a smaller basis.\nCost of LLL # The theoretical bound on the quality of the LLL is $\\delta^d = (\\frac{4}{3})^\\frac{d-1}{4}$ [1] leading to approximately $\\delta \\approx 1.075$. In practice, we get much better results on average, empirically about $\\delta \\approx 1.021$ .\nIn terms of runtime, we will consider a heuristic bound (which better approximates empirical results) of $O(d^3 \\log^2(B))$.\nBKZ algorithm # The Block Korkine-Zolotarev (BKZ) algorithm is a lattice reduction algorithm that generalizes the LLL algorithm to achieve stronger reduction properties. The BKZ algorithm is defined as a blockwise reduction algorithm that iteratively applies a form of lattice basis reduction to overlapping blocks of vectors within the basis. The assumption made in its analysis is that iterative blocks taken behaves like a random lattice. It is in fact a relaxation of the Hermite-Korkine-Zolotarev (HKZ) reduction. The HKZ reduction is a stronger form of lattice reduction that ensures each vector in the basis is the shortest vector in the lattice projected orthogonally onto the space spanned by the preceding basis vectors.\nHKZ reduction # Let b₁, b₂, \u0026hellip;, bₙ be a basis of a lattice L.\nThe basis is said to be HKZ-reduced if:\nb₁ is the shortest vector in the lattice L. For i = 2, 3, \u0026hellip;, n, the vector bᵢ is the shortest vector in the lattice L projected orthogonally onto the span of b₁, b₂, \u0026hellip;, bᵢ₋₁. BKZ # Assuming we have an SVP oracle, the BKZ algorithm is defined as follows:\nData: LLL-reduced basis B (preprocessed) and block size beta repeat until no changes for k in 0 to d-2 LLL on local projected block B[k, ..., min(k+beta, d)] v \u0026lt;-- SVP-Oracle(local projected block B[k, ..., min(k+beta, d)]) insert v into B at index k and handle linear dependencies with LLL end In practice, stronger preprocessing than LLL is often performed before the BKZ block loop. Hereafter is a simple representation of a block moving through a matrix for one tour of BKZ, in practice we often do several tours because we are supposed to stop when no more significant changes occur.\nExample of the BKZ algorithm block sliding. Theoretically, a BKZ-$\\beta$ reduced basis satisfies, for $\\epsilon \u0026gt; 0$:\n$$\\lVert \\bold{b_0} \\rVert \\leq \\sqrt{(1 + \\epsilon) \\gamma_{\\beta}}^{(\\frac{d-1}{\\beta - 1} + 1)} Vol(\\Lambda(\\bold{B}))$$$$\\gamma_\\beta = \\sup \\{ \\lambda_1(\\Lambda) | \\Lambda \\in \\mathbb{R}^\\beta, Vol(\\Lambda) = 1 \\}$$Several variants of BKZ exist, the one giving asymptotically the best worst-case guarantees is the Slide reduction described in [8] and it achieves\n$$\\lVert \\bold{b_0} \\rVert \\leq \\sqrt{(1 + \\epsilon) \\gamma_{\\beta}}^\\frac{d-1}{\\beta - 1} Vol(\\Lambda(\\bold{B}))$$By combining the gaussian heuristic and the definition of a BKZ-$\\beta$ reduced basis, we arrive again at the geometric assumption, which states that the log-lengths of reduced vectors follow a geometric series (which we can plot as a line as we did for LLL). This time however, it depends on the block-size chosen to run BKZ.\nWe can write\n$$\\log(\\lVert \\bold{b_i}^*\\rVert) = \\frac{d - 1 - 2i}{2}\\log(\\alpha_\\beta) + \\frac{1}{d}\\log(Vol(\\Lambda))$$where $\\alpha_\\beta$ is the slope under the geometric assumption that can be calculated from the gaussian assumption as\n$$\\alpha_\\beta = \\sqrt{\\frac{d}{2\\pi e}}^\\frac{2}{\\beta - 1}$$This result from [6] is reasonably accurate only if $d\\gg\\beta$ and $\\beta \u0026gt; 50$, which is why we will use fixed estimates for small dimensions (also, small dimension can be directly solved by an exact solver).\nCost of BKZ # Costing BKZ means having a good idea of the impact of the block-size on the quality of our reduced basis. For this, we could either make the approximation $\\delta_\\beta \\approx \\sqrt \\alpha_\\beta$ or use the following limit defined in [9] that works well for $\\beta \u0026gt; 50$ and typical $d$ used in cryptography [6]. $$\\lim_{\\beta\\rightarrow\\infty}\\delta_\\beta = (\\frac{\\beta}{2\\pi e}(\\pi\\beta)^\\frac{1}{\\beta})^\\frac{1}{2(\\beta - 1)}$$and write for SIS\n$$\\lVert \\bold{b_1} \\rVert \\approx \\delta_\\beta^{d-1} Vol(\\Lambda)^{\\frac{1}{d}}$$ Costing BKZ as a whole is complicated because we do not know how many tours we will have to run, which means we don\u0026rsquo;t really know in advance the number of SVP-Oracle calls we will have to make. Furthermore, many improvements on plain BKZ have been made when some techniques are used as a subroutine for the oracle (for example extreme pruning in the context of enumeration), which makes security estimates done via lattice reduction very sensitive to many factors. Also, local preprocessing techniques have been introduced as part of the algorithm in a variant of BKZ known as progressive BKZ. To make our tool comparable to the lattice estimator by [10], we will follow the same simplifying assumption and consider a consistent 8 tours of BKZ. This makes sense following experimental results that showed that most progress is made in the 7-9 first tours. We will then use: $$cost = \\tau \\cdot d \\cdot T_{SVP}$$ where the number of BKZ tours we do $\\tau$ is considered to be 8. The number of times the SVP oracle is called per tour, which is about the dimension of the lattice d The cost of the SVP oracle is $T_{SVP}$ Refinements # A good history of the refinements that have been made about BKZ runtime and the way we assume security in problems based on lattice reduction can be found in [6]. Indeed, it appears some of our assumptions are not entirelly true, especially GSA. In the estimator, most refinements are either directly modelled into a new simulator for the GSA shape or through adjusted cost models of SVP oracles. The way we cost BKZ as a whole stays the same.\nThe first GSA lie # If we think more about the way we slide the block through the matrix during lattice reduction, we can easily come to the conclusion that something different must happen at the end. Indeed, the GSA assumptions ignores what happens for the last $d-\\beta$ coordinates. This last block is in fact HKZ reduced and we can therefore adapt the tail of our assumption leading to Tail-adapted GSA (TGSA).\nLet us use the following definition for the HKZ-shape in dimension d. For $i=0, \\ldots , d-1:$\n$$ h_i = \\log\\left(\\sqrt{\\frac{d - i}{2\\pi e}}\\right) - \\frac{1}{d-1} \\sum_{j \u003c i} h_j, $$Now using this in modifying our BKZ GSA assumed shape we can get the following two parts equation:\n$$ \\log\\left(\\|b_i^*\\|\\right) = \\begin{cases} \\frac{d - 1 - 2i}{2} \\log(\\alpha \\beta) + s, \u0026 \\text{for } 0 \\leq i \\leq d - \\beta, \\\\ h_{i-(d-\\beta)} + \\log\\left(\\|b_{d-\\beta}^*\\|\\right) - h_0, \u0026 \\text{for } d-\\beta \\leq i \u003c d. \\end{cases} $$ The second GSA lie and Z(T)GSA # Geometric series assumptions (tail-adapted or not) have been shown to be a bad choice for small dimensions (think 50 and below) and when $d$ is a multiple of $\\beta$. Furthermore, these assumptions only give an estimate about the size after the algorithm is finished and not the evolution of vectors through it. We will here just mention that an estimator has been introduced in [3] to take these observations into accounts. Thanks to its implementation in FPyLLL, we will make use of it to calculate the cost of BKZ for small values (by simply hardcoding the needed values).\nBecause we will work with q-ary lattices, they will always contain vector $(q, 0, \\ldots, 0)$ and its permutation. These vectors can be considered short in certain circonstances and shorter that what GSA would predict. A ZGSA assumption adapted to such lattices is possible, but it remains unsure what such an assumption would look like on block reduction algorithms.\nBKZ 2.0 improvements # BKZ 2.0 [3] introduces several enhancements to the traditional BKZ algorithm, improving its efficiency and the quality of lattice reduction. Here, we summarize the key improvements:\nIntroduction of Pruning:\nBKZ 2.0 incorporates sound pruning and extreme pruning, techniques introduced by Gama, Nguyen, and Regev. These pruning methods reduce the size of the enumeration tree by removing branches with negligible probability of success. Preprocessing of Local Blocks:\nBKZ 2.0 ensures local bases are better reduced than standard LLL-reduction before enumeration. This preprocessing step reduces the cost of enumeration by improving the quality of the local basis. For a local projected lattice \\( L[j,k] \\), preprocessing increases the volumes of projected lattices \\( L[k-d+1,k] \\), reducing the size of the enumeration tree. Optimizing the Enumeration Radius:\nBKZ 2.0 optimizes the initial radius \\( R \\) for enumeration to avoid unnecessary computation. The optimized radius is calculated as: $$ R = \\min\\left(\\sqrt{\\gamma} \\cdot \\text{GH}(L[j,k]), \\|b_j^*\\|\\right), $$ where \\( \\text{GH}(L[j,k]) \\) is the Gaussian heuristic for the local block and \\( \\gamma \\approx 1.1 \\). Simulation for High Block Sizes:\nBKZ 2.0 predicts output quality and running time for high block sizes (\\( \\beta \\geq 50 \\)) through a simulation algorithm. This includes: Predicting the Gram-Schmidt sequence \\( \\|b_i^*\\| \\) during BKZ 2.0 reduction. Estimating the block sizes required to achieve a target Hermite factor. References # P. Nguyen,\u0026#32;Hermite’s constant and lattice algorithms,\u0026#32;in The LLL algorithm: Survey and applications,\u0026#32;Springer, 2009, pp. 19–69.↩P. Nguyen and\u0026#32;B. Vallee,\u0026#32;The LLL algorithm.\u0026#32;Springer, 2010.\u0026#32;↩Y. Chen and\u0026#32;P. Nguyen,\u0026#32;BKZ 2.0: Better lattice security estimates,\u0026#32;In Proc. International conference on the theory and application of cryptology and information security,\u0026#32;2011, pp. 1–20.\u0026#32;↩N. Gama,\u0026#32;N. Howgrave-Graham,\u0026#32;H. Koy, and\u0026#32;P. Nguyen,\u0026#32;Rankin’s constant and blockwise lattice reduction,\u0026#32;In Proc. Advances in cryptology-CRYPTO 2006: 26th annual international cryptology conference, santa barbara, california, USA, august 20-24, 2006. Proceedings 26,\u0026#32;2006, pp. 112–130.\u0026#32;↩A. Lenstra,\u0026#32;H. Lenstra, and\u0026#32;L. Lovász,\u0026#32;Factoring polynomials with rational coefficients,\u0026#32;Mathematische annalen,\u0026#32;vol. 261,\u0026#32;pp. 515–534,\u0026#32;1982.\u0026#32;↩M. Albrecht and\u0026#32;L. Ducas,\u0026#32;Lattice attacks on NTRU and LWE: A history of refinements,\u0026#32;Cryptology ePrint Archive,\u0026#32;2021.\u0026#32;↩N. Gama and\u0026#32;P. Nguyen,\u0026#32;Predicting lattice reduction,\u0026#32;In Proc. Advances in cryptology–EUROCRYPT 2008: 27th annual international conference on the theory and applications of cryptographic techniques, istanbul, turkey, april 13-17, 2008. Proceedings 27,\u0026#32;2008, pp. 31–51.\u0026#32;↩N. Gama and\u0026#32;P. Nguyen,\u0026#32;Finding short lattice vectors within mordell’s inequality,\u0026#32;In Proc. Proceedings of the fortieth annual ACM symposium on theory of computing,\u0026#32;2008, pp. 207–216.\u0026#32;↩Y. Chen,\u0026#32;Reduction de reseau et securite concrete du chiffrement completement homomorphe.\u0026#32;Paris 7, 2013.\u0026#32;↩M. Albrecht,\u0026#32;R. Player, and\u0026#32;S. Scott,\u0026#32;On the concrete hardness of learning with errors.\u0026#32;Cryptology ePrint Archive, Paper 2015/046, 2015.\u0026#32;[Online]. Available: https://eprint.iacr.org/2015/046\u0026#32; ↩ "},{"id":4,"href":"/lattirust_estimator.io/docs/sis/","title":"SIS","section":"Docs","content":" Short Integer Solution problem (SIS) # Let us look at two different ways to define m-dimensional q-ary lattices from a matrix \\(\\bold{A} \\in \\mathbb{Z}_q^{h\\times w}\\). We cran write:\n$$\\Lambda_q(\\bold{A}) = \\{\\bold{y} \\in \\mathbb{Z}^w : \\bold{y} = \\bold{A}^T\\bold{s} \\text{ mod q }\\text{ for some } \\bold{s} \\in \\mathbb{Z}^h\\}$$$$\\Lambda^T_q(\\bold{A}) = \\{\\bold{y} \\in \\mathbb{Z}^w : \\bold{A}\\bold{y} = \\bold{0} \\text{ mod q }\\}$$The first lattice, \\( \\Lambda_q(\\boldsymbol{A}) \\), is generally referred to as the primal problem and is closely related to the Learning With Errors (LWE) problem. Solving the LWE problem involves finding a short vector in \\( \\Lambda_q(\\boldsymbol{A}) \\).\nLWE Problem Statement # Given:\nA matrix \\( \\mathbf{A} \\in \\mathbb{Z}_q^{h \\times w} \\), where entries are chosen uniformly at random, A secret vector \\( \\mathbf{s} \\in \\mathbb{Z}_q^w \\), chosen uniformly at random, An error vector \\( \\mathbf{e} \\in \\mathbb{Z}_q^h \\), where each entry is drawn from an error distribution \\( \\chi \\) (e.g., a discrete Gaussian distribution), A vector \\( \\mathbf{b} = \\mathbf{A} \\mathbf{s} + \\mathbf{e} \\mod q \\). To find:\nIn the search version: The secret vector \\( \\mathbf{s} \\in \\mathbb{Z}_q^w \\), In the decision version: Distinguish between: Vectors \\( (\\mathbf{A}, \\mathbf{b}) \\) generated as described above, and Vectors \\( (\\mathbf{A}, \\mathbf{b}) \\) where \\( \\mathbf{b} \\) is chosen uniformly at random from \\( \\mathbb{Z}_q^h \\). We often assume that the error distribution \\( \\chi \\) has small standard deviation compared to \\( q \\).\nThe second lattice, \\( \\Lambda_q^T(\\boldsymbol{A}) \\), is generally referred to as the dual problem and is closely related to the Short Integer Solution (SIS) problem. Solving the SIS problem involves finding a short vector in \\( \\Lambda_q^T(\\boldsymbol{A}) \\).\nSIS Problem Statement # Given:\nA matrix \\( \\boldsymbol{A} \\in \\mathbb{Z}_q^{h \\times w} \\), where entries are uniformly sampled from \\( \\mathbb{Z}_q \\), An integer bound \\( \\beta \u003e 0 \\). To find:\nA nonzero vector \\( \\boldsymbol{y} \\in \\mathbb{Z}^w \\) such that: $$ \\boldsymbol{A} \\boldsymbol{y} = \\boldsymbol{0} \\mod q \\quad \\text{and} \\quad \\|\\boldsymbol{y}\\| \\leq \\beta $$ We often assume that the entries of \\( \\boldsymbol{A} \\) are uniformly distributed over \\( \\mathbb{Z}_q \\).\nFormal definitions # Firstly, let us assume that q is always prime and that $w\u0026gt;h$. We can state the following definitions.\nDefinition 1 : We define \\(SIS(h, w, q, \\beta, p)\\) as follows. Given \\(\\bold{A} \\in \\mathbb{Z}_q^{h\\times w}\\) find the short vector \\(\\bold{s} \\in \\mathbb{Z}^w\\) where \\(0 \u0026lt; \\lVert s \\rVert_p \\leq \\beta\\) and $q=\\text{poly}(h)$, $m=\\text{poly}(h)$, $\\beta=\\text{poly}(h)$.\nWe can note that the problem becomes trivial as soon as \\(\\beta \\geq q\\), no matter the norm used. Also, for the problem not to be vacuous (no possible solutions), we need the select $\\beta$ big enough. An often used bound is $\\beta \u0026gt; \\sqrt{h\\log(q)}$\nDefinition 2: The inhomogeneous SIS problem is called \\(ISIS(h, q, w, \\beta, p, t)\\) and is defined as follows. Given \\(\\bold{A} \\in \\mathbb{Z}_q^{h \\times w}\\) and a target vector \\(\\bold{t} \\in \\mathbb{Z}_q^h\\), find a short vector \\(\\bold{s} \\in \\mathbb{Z}^w\\) such that \\(\\bold{A} \\bold{s} = \\bold{t} \\mod q\\) and \\(0 \u0026lt; \\lVert \\bold{s} \\rVert_p \\leq \\beta\\). Also here $q=\\text{poly}(h)$, $m=\\text{poly}(h)$, $\\beta=\\text{poly}(h)$.\nLemma 1: SIS and ISIS are considered asymptotically equivalent problem (a reduction exists from one to the other) [1].\nLemma 2: Solving SIS on $A^T$ directly allows to solve LWE on $A$. They are also considered asymptotically equivalent problems. A reduction exists from SIS to LWE. In the other direction, a quantum reduction exists.\nThe following theorems are the fundamental theorems that allows us to relate an SIS instance to a known very hard problem (SVP-like).\nTheorem 1: For any polynomially bounded $w$, $\\beta = \\text{poly}(h)$, and any prime $q \\geq \\beta \\cdot \\omega(\\sqrt{h \\log h})$, the average-case problems $SIS(h, q, w, \\beta)\\text{ and } ISIS(h, q, w, \\beta)$ are at least as hard as approximating the $SIVP$ problem (among others) in the worst case within a factor of $\\gamma = \\beta \\cdot \\tilde{O}(\\sqrt{n})$ [2].\nThis result has been improved for smaller value of the q parameters (nearly equal to the length bound $\\beta$).\nTheorem 2: For any polynomially bounded $w$, $\\beta = \\text{poly}(h)$, and any prime $q \\geq \\beta \\cdot h^\\delta$, $\\delta\u0026gt;0$ some constant, the average-case problems $SIS(h, q, w, \\beta)\\text{ and } ISIS(h, q, w, \\beta)$ are at least as hard as approximating the $SIVP$ problem (among others) in the worst case within a factor of $\\gamma = \\beta \\cdot \\tilde{O}(\\sqrt{n})$ [3].\nThis means that to estimate the security given by an SIS instance on which a cryptographic primitive relies, we can estimate the security of solving(or attacking) an SVP-instance. In the following we will present an high overview of possible attacks, and the one we choose to base our security estimates on.\nAttacks on SIS # Lattice reduction based attacks # Firstly, let\u0026rsquo;s mention lattice reduction that we have already broadly covered, so we use lattice reduction to directly find short vectors. This is the approach we take when estimating the hardness of an SIS instance and we will provide further details on the exact steps for the L-2 and L-infinity norms later on.\nCombinatorial attacks # Combinatorial attacks to solve were proposed in [4]. They work as follows, given matrix $\\bold{A} \\in \\mathbb{Z}_q^{h \\times w}$ and the length bound $b$:\nDivide Columns into Groups: Split the columns of \\( \\boldsymbol{A} \\) into \\( 2^k \\) groups, each containing \\( \\frac{w}{2^k} \\) columns, where \\( k \\) is a parameter to be determined. Generate Initial Lists: For each group, construct a list containing all linear combinations of the columns with coefficients in \\( \\{-b, \\dots, b\\} \\). Each list contains \\( L = (2b + 1)^{w / 2^k} \\) vectors in \\( \\mathbb{Z}_q^h \\). Combine Lists in Pairs: Iteratively combine the lists in pairs. For two lists, compute all sums \\( \\boldsymbol{x} + \\boldsymbol{y} \\), retaining only those vectors whose first \\( \\log_q L \\) coordinates are zero. This reduces the size of the resulting list to approximately \\( L \\). Repeat Combination: Continue combining lists until a single list remains after \\( k \\) iterations. The final list contains vectors that are zero in their first \\( k \\cdot \\log_q L \\) coordinates. Extract the Short Vector: The final list contains vectors that are zero in all but their last \\( h - k \\cdot \\log_q L \\approx \\log_q L \\) coordinates. With appropriate parameter choice k, the list is expected to include the all-zero vector, which is a combination of the columns of $\\bold{A}$ bounded by $b$ so we can expect to find a short vector. Algorithms based on this combinatorial attacks but on LWE have also been proposed like BKW [5] and Coded-BKW [6]. Combinatorial attacks work well for small dimensions but become quickly unfeasible for larger dimensions. However, one good aspects of combinatorial methods is that they actually exploit the large number of columns \\( w \\) in \\( \\boldsymbol{A} \\), which lattice reduction do not.\nVia LWE # In the case of solving SIS through LWE, several additional attacks can be considered, such as bounded distance decoding (BDD) [7], Arora-Ge attacks [8], and meet-in-the-middle (MITM) [9] approaches. BDD focuses on finding the closest lattice point to a given target, leveraging the fact that LWE can be reduced to a closest vector problem (CVP) in certain cases. Arora-Ge attacks use algebraic techniques to solve LWE by reducing it to a system of polynomial equations, though their practicality diminishes with high noise. MITM attacks exploit combinatorial techniques to reduce the effective complexity of solving LWE by balancing the time and memory trade-offs. As SIS is our focus, we will not delve deeply into these methods.\nIn the following analysis, we will present the concrete method used to estimate the security of an SIS instance. We will separate the use of the euclidean norm and the infinity norm, using the theory we built in the previous sections (lattice reduction and cost models). What we actually reduce te security to is an H-SVP solver.\nL2 norm strategy # For the L2 norm strategy, we follow the steps depicted in [4] and [10]. In the following you can assume all log are base 2, except mentioned otherwise. As a first step, we ensure the parameters given don\u0026rsquo;t solve trivially or actually accept solutions by checking that $\\beta \u0026lt; q$ and $\\beta \\geq w\\log(q)$.\nFinding the optimal lattice shape for reduction # Given a q-ary lattice \\(\\bold{A} \\in \\mathbb{Z}_q^{h\\times w}\\), we can then say with high probability that the rows of $\\bold{A}$ are independent over $\\mathbb{Z}_q$. As a result of this, the lattice $\\Lambda^T_q(\\bold{A})$ has $q^{w-h}$ points in $\\mathbb{Z}^w_q$. This leads to the volume or determinant of the matrix to be $Vol(\\Lambda^T_q(\\bold{A})) = q^{h}$. Using the gaussian heuristic from the lattice reduction section, we can express\n$$\\lambda_1(\\Lambda^T_q(\\bold{A})) \\approx q^\\frac{h}{w}\\sqrt{\\frac{w}{2\\pi e}}$$as an estimate of the lenght of the smallest vector.\nIt has been experimentally observed that the length of the vector obtained by the best known lattice reduction algorithms on a random w-dimensional q-ary lattice is close to $\\min(q, q^\\frac{h}{w}\\delta^w)$. We can also observe that increasing the $w$ parameter does not make the problem any harder. In fact, we can fix the parameter $w$ by completely letting go a certain number of columns. By plotting $q^{\\frac{h}{w}}\\delta^w$ as a function of w, the minimum as been determined to be $2^{2\\sqrt{h\\log q \\log \\delta}}$ for $w = \\sqrt{h\\log q / \\log \\delta}$. So we will always do a lattice reduction over a matrix of size $h\\times w\u0026rsquo;$, where\n$$w' = \\sqrt{\\frac{h\\log q}{\\delta}}$$Indeed, for smaller w the lattice becomes too sparse and does not contain enough vectors to have small ones, and bigger w actually prevents lattice reduction to perform optimally. This leads us to a shortest vector found of length $\\min(q, 2^{2\\sqrt{h\\log q \\log \\delta}})$. In the calculation for the optimal $w\u0026rsquo;$, we use the theoretical value for $\\delta$ and replace the bound of the smallest vector by $\\beta$ because we want a vector that is smaller or equal to $\\beta$.\n$$\\lambda_1(\\Lambda^T_q(\\bold{A})) = q^\\frac{h}{w}\\delta^w \\Rightarrow \\beta = q^\\frac{h}{w}\\delta^w $$$$\\log \\beta = w \\log \\delta + \\frac{h}{w}\\log q \\Rightarrow \\log \\delta = \\frac{\\log \\beta}{w} - \\frac{h\\log q}{w^2}$$Replacing w by w\u0026rsquo; we get the theoretical\n$$ \\log\\delta = \\frac{\\log^2\\beta}{4h\\log q} $$ Solve for the required hermite factor # Now that we have the optimal dimension on which to apply our lattice reduction via BKZ, we need to define what the best block-size would be. We know from previously that BKZ achieves\n$$\\beta \\approx \\delta_\\beta^{w'-1} Vol(\\Lambda)^{\\frac{1}{w'}}$$and by playing around with the equation, we know that our target hermite factor will be\n$$\\beta^\\frac{1}{w'-1} = \\delta Vol(\\Lambda)^{\\frac{1}{w'(w' - 1)}} \\Rightarrow \\delta \\approx \\beta \\frac{1}{Vol(\\Lambda)^{1/w'}}^\\frac{1}{w'-1}$$$$\\log\\delta = \\frac{1}{w'-1}(\\log\\beta - \\frac{h}{w'}\\log q)$$We check that this found $\\delta$ makes sense by checking that it is bigger than one. Now that we know what we aim for in terms of hermite factor, the next question is what kind of block-size gives us such a guarantee. For this, we follow [11] and retake their manually computed $\\delta_\\beta$ values for small $\\beta$ under 40 (that were computed using fpyll) and for bigger block-sizes, we use\n$$\\lim_{\\beta\\rightarrow\\infty}\\delta_\\beta = (\\frac{\\beta}{2\\pi e}(\\pi\\beta)^\\frac{1}{\\beta})^\\frac{1}{2(\\beta - 1)}$$Once we know the optimal $\\beta$, we can check that it also makes sense by checking it is under our matrix size (so under $w\u0026rsquo;$). If it is, we can go ahead and estimate the actual cost of running BKZ for block-size $\\beta$ on a matrix of dimension $\\bold{A}\\in \\mathbb{Z}_q^{h \\times w\u0026rsquo;}$.\nEstimating the security by estimating the BKZ cost # $$cost = \\tau \\cdot d \\cdot T_{SVP}$$ where\nthe number of tours we do $\\tau$ is considered to be 8. The number of times the SVP oracle is called per tour, which is about the dimension of the lattice The cost of the SVP oracle is $T_{SVP}$ The cost of the SVP oracle is defined by the internal cost that the user will select.\nL-inf norm strategy # As in [11], we separate between the Matzov and the Kyber analysis. Both however follow the same high-level strategy. First, we evaluate the security for the L2-bound as explained previously.This will act as a lower bound on the hardness of our infinity bound problem. The strategy is then to analyze the probability of obtaining a vector that respects the infinity bound constraints on all coordinates. When $\\sqrt{w}\\beta \u0026lt; q$ we apply the analysis in [12] and when $\\sqrt{w}\\beta \\geq q$, we apply the analysis in [13]. The attack is composed on 2 parts. First, we use lattice reduction to find many short vectors of the lattice. Then, we assume some (Kyber analysis) or all (Matzov analysis) follow independent gaussian distributions and analyze the probability of obtaining at least one short vectors that respects the infinity bound on all coordinates. The difference in the style of analysis is brought down to the assumptions that we make about our vectors after sampling them.\nMatzov analysis # Matzov introduces in his paper an enhanced way to sample many short vectors from running BKZ. First, the algorithm runs BKZ with a block-size $\\beta_1$ to find a reduced basis of $\\Lambda$. In this phase, we apply the dimension-for-free technique [14] since we only wish for a reduced basis. Then as a second step, we perform sieving with another block-size $\\beta_2$ without the dimension-for-free technique because in this step we want as output as many short vectors as possible and will use all of them.\nThe algorithm can be summarized as\nShort Vectors Sampling Procedure\nInput: A basis $B = (b_1, \\ldots, b_d)$ for a lattice, integers $\\beta_1$, $\\beta_2 \\leq d$, and the desired number of short vectors $D$. Let $N_{sieve}(\\beta_2)$ be the number of vectors after performing lattice sieving with a block-size of $\\beta_2$. Output: A list of at least $D$ short vectors from the lattice. For $i = 1, \\ldots, \\left\\lceil \\frac{D}{N_{\\text{sieve}}(\\beta_2)}\\right\\rceil$: Randomize the basis $B$. Run BKZ$_{d, \\beta_1}$ on $B$ to obtain a reduced basis $(b\u0026rsquo;_1, \\ldots, b\u0026rsquo;_d)$. Run a sieve of dimension $\\beta_2$ on the sublattice $$(b'_1, \\ldots, b'_{\\beta_{2}})$$ to obtain a list of vectors and add them to $L$. Return $L$. Note that a similar procedure was presented in [15]. We consider the cost of this algorithm to be\n$$ \\lceil\\frac{D}{N_{\\text{sieve}}(\\beta_2)}\\rceil(T_{BKZ(d, \\beta_1)} + T_{sieve}(\\beta_2)) $$also, the expected length of vectors generated by the algorithm can be approximated asymptotically as\n$$ \\ell \\approx \\sqrt{\\frac{4}{3}} \\cdot \\sqrt{\\frac{\\beta_2}{2\\pi e}}\\delta(\\beta_1)^{\\frac{d-\\beta_2}{2}} $$Most importantly, we then make the assumption that if $\\ell$ is the expected length of the vectors from the short vectors sampling procedure, then the coordinates if the returned vectors have approximately independent Gaussian distributions with mean 0 and standard deviation $\\frac{\\ell}{ \\sqrt{d}}$. This means that if we want the probability of $d$ vectors respecting the infinity bound, we can compute it via:\n$$p = (2*\\Phi(\\text{length bound}_{\\infty}) - 1) ^ d \\text{ where } \\Phi \\sim \\mathcal{N}(0, \\frac{\\ell}{ \\sqrt{d}})$$Now when we find a block size and a number of dimension for which we have a probability of finding such vectors, then we simply repeat the experiment until we get the success probability that we want. As a generalization and to not overwhelm the user with custumization possibilities, we fix the overall probability of success to attain to 0.99. Also, for most parameters that are useful in cryptography, we follow one remark of the original authors that approximate $N_{\\text{sieve}}(\\beta_2)\\approx D$.\nWe also use the same assumption of the paper for the cost of progressive sieving and of a progressive BKZ, which are\n$ T_{sieve}(\\beta) = \\frac{1}{1-2^{-0.292}} * T_{sieving subroutine}(\\beta) $\n$ T_{BKZ}(d, \\beta) = (\\frac{1}{1-2^{-0.292}})^2(d-\\beta+1) * T_{sieving subroutine}(\\beta_{eff}) $\nwhere $\\beta_{eff}$ comes from the dimension for free calculation, where to solve SVP with sieving in $\\beta$ you only need to do it in $\\beta_{eff} = \\beta - \\frac{\\beta\\log(4/3)}{\\log(\\beta/2\\pi e)}$. The paper also gives new sieving costs estimates with an improved enumeration routine.\nKyber analysis # In this analysis, we don\u0026rsquo;t consider only the GSA but we consider the q-ary structure of the lattice. As such, we consider that BKZ could now produce q-vectors at the given length bound. Only the middle region of the basis after lattice reduction will produce gaussian independent vectors as in the Matzov analysis. This is why when sampling our vectors, we will only keep the vectors starting from the zone where they are not q-ary and ending before they all becomes units. To account for the q-ary structure, the simulator used is automatically ZGSA or if need be you can use LGSA that forgets q-vectors by using rerandomization. The standard deviation used to calculate the probability of vectors respecting our infinity bound is then corrected to $\\frac{l}{d\u0026rsquo;}$ where d\u0026rsquo; is the number if coordinates that are gaussians after we remove q-vectors. The probability and costs are then computed similarly as in Matzov.\nIterating over the dimenstion and block-sizes # Now that we have a cost for evaluating the infinity norm for a specified block-size and number of columns, we still need to find the optimal parameters. In contrary to the L2 setting, we have no theoretical idea what would be the preferred number of columns to use to solve the underlying SIS instance or the best block-size. As such, we made the choice to iterate over all possible dimensions and blocksizes in a grid-search manner. We use the optimal number of columns for the L2 norm and the corresponding optimal block-size for the L2 norm as an upperbound on our block-size seach and go through all possible number of columns. Given the possibly large search space, we opted for an adaptive grid seach that first coarsely goes through the grid followed by a much finer grid seach on a restricted space. The returned cost is the result of this grid seach evalutation.\nReferences # V. Lyubashevsky,\u0026#32;Lattice signatures without trapdoors,\u0026#32;In Proc. Annual international conference on the theory and applications of cryptographic techniques,\u0026#32;2012, pp. 738–755.\u0026#32;↩C. Gentry,\u0026#32;C. Peikert, and\u0026#32;V. Vaikuntanathan,\u0026#32;Trapdoors for hard lattices and new cryptographic constructions,\u0026#32;In Proc. Proceedings of the fortieth annual acm symposium on theory of computing,\u0026#32;2008, pp. 197–206.\u0026#32;↩D. Micciancio and\u0026#32;C. Peikert,\u0026#32;Hardness of sis and lwe with small parameters,\u0026#32;In Proc. Annual cryptology conference,\u0026#32;2013, pp. 21–39.\u0026#32;↩D. Micciancio and\u0026#32;O. Regev,\u0026#32;Lattice-based cryptography,\u0026#32;in Post-quantum cryptography,\u0026#32;Springer, 2009, pp. 147–191.↩A. Blum,\u0026#32;A. Kalai, and\u0026#32;H. Wasserman,\u0026#32;Noise-tolerant learning, the parity problem, and the statistical query model,\u0026#32;Journal of the ACM (JACM),\u0026#32;vol. 50,\u0026#32;no. 4,\u0026#32;pp. 506–519,\u0026#32;2003.\u0026#32;↩Q. Guo,\u0026#32;T. Johansson, and\u0026#32;P. Stankovski,\u0026#32;Coded-bkw: Solving lwe using lattice codes,\u0026#32;In Proc. Annual cryptology conference,\u0026#32;2015, pp. 23–42.\u0026#32;↩R. Lindner and\u0026#32;C. Peikert,\u0026#32;Better key sizes (and attacks) for lwe-based encryption,\u0026#32;In Proc. Topics in cryptology–ct-rsa 2011: The cryptographers’ track at the rsa conference 2011, san francisco, ca, usa, february 14-18, 2011. Proceedings,\u0026#32;2011, pp. 319–339.\u0026#32;↩S. Arora and\u0026#32;R. Ge,\u0026#32;New algorithms for learning in presence of errors,\u0026#32;In Proc. International colloquium on automata, languages, and programming,\u0026#32;2011, pp. 403–415.\u0026#32;↩S. Bai and\u0026#32;S. Galbraith,\u0026#32;Lattice decoding attacks on binary lwe,\u0026#32;In Proc. Information security and privacy: 19th australasian conference, acisp 2014, wollongong, nsw, australia, july 7-9, 2014. Proceedings 19,\u0026#32;2014, pp. 322–337.\u0026#32;↩M. Albrecht and\u0026#32;L. Ducas,\u0026#32;Lattice attacks on ntru and lwe: A history of refinements,\u0026#32;Cryptology ePrint Archive,\u0026#32;2021.\u0026#32;↩M. Albrecht,\u0026#32;R. Player, and\u0026#32;S. Scott,\u0026#32;On the concrete hardness of learning with errors.\u0026#32;Cryptology ePrint Archive, Paper 2015/046, 2015.\u0026#32;[Online]. Available: https://eprint.iacr.org/2015/046\u0026#32; ↩MATZOV,\u0026#32;Report on the security of lwe: Improved dual lattice attack.\u0026#32;Zenodo, 2022.\u0026#32;doi:10.5281/zenodo.6493704↩V. Lyubashevsky,\u0026#32;L. Ducas,\u0026#32;E. Kiltz,\u0026#32;T. Lepoint,\u0026#32;P. Schwabe,\u0026#32;G. Seiler,\u0026#32;D. Stehlé, and\u0026#32;S. Bai,\u0026#32;Crystals-dilithium,\u0026#32;Algorithm Specifications and Supporting Documentation,\u0026#32;2020.\u0026#32;↩L. Ducas,\u0026#32;Shortest vector from lattice sieving: A few dimensions for free,\u0026#32;In Proc. Annual international conference on the theory and applications of cryptographic techniques,\u0026#32;2018, pp. 125–145.\u0026#32;↩Q. Guo and\u0026#32;T. Johansson,\u0026#32;Faster dual lattice attacks for solving lwe with applications to crystals,\u0026#32;In Proc. Advances in cryptology–asiacrypt 2021: 27th international conference on the theory and application of cryptology and information security, singapore, december 6–10, 2021, proceedings, part iv 27,\u0026#32;2021, pp. 33–62.\u0026#32;↩ "},{"id":5,"href":"/lattirust_estimator.io/docs/rsis-msis/","title":"Module SIS and Ring SIS","section":"Docs","content":" Compact versions of SIS # In practice, basic SIS leads to large key sizes and large parameters overall. Thinking about $\\bold{A} \\in \\mathbb{Z}_q^{h\\times w}$ with $m \u0026gt; n$ leads to at least quadratic complexity. This is why more compact version have been introduced like Ring-SIS and Module-SIS. By using such underlying structures, we are able to obtain much smaller key sizes in the schemes that use an SIS instance as the basis for their security. As a refresher on the module and ring structure the reader can consult [1]. Consider that we choose some $h = 2^k$ and that we choose the columns of $\\bold{A}$ to be elements of the ring $\\mathbb{Z}_q / \\langle x^h + 1 \\rangle$, if $h$ is a power of two it is irreducible over the rationals, then we need to precise less elements to describe the same SIS instance. Using a module, we can get an even compacter structure.\nRing SIS # Definition:[2]\nWe define the Ring Short Integer Solution (Ring-SIS) problem as follows $RSIS(h, w, q, \\beta, p)$. Let:\n\\( R_q = \\mathbb{Z}_q / \\langle x^h + 1 \\rangle \\), where \\( h \\) is a power of two (ensuring \\( x^h + 1 \\) is irreducible over \\( \\mathbb{Q} \\)), Given $a_1, \\ldots, a_w$ in R_q, chosen independently from the uniform distribution, find $s_1, \\ldots, s_w \\in R$ such that:\n$$ \\sum_{i=1}^{w}a_i s_i = 0 \\pmod{q} \\text{ and } 0 \u003c \\|\\mathbf{s}\\|_p \\leq \\beta $$where $\\bold{s} = (s_1, \\ldots, s_w)^T \\in R^m$.\nEach ring element $r\\in R$ can be interpreted as an $h$-dimensional vector with coefficients $r_i$ such that $r = \\sum_{i=0}^{n-1}r_ix^i$. Now if we compare RSIS to SIS, each matrix element $a_i$ in RSIS actually actually corresponds to the $h\\times h$ nega-circulant matrix. In this setup, R-SIS is just a variant of SIS where $\\bold{A}$ is restricted to being block negacirculant $\\bold{A}= [Rot(a_1)|\\ldots|Rot(a_n)]$ where Rot(b) is defined as\n$$ \\text{Rot}(b) = \\begin{bmatrix} b_0 \u0026 -b_{h-1} \u0026 -b_{h-2} \u0026 \\cdots \u0026 -b_1 \\\\ b_1 \u0026 b_0 \u0026 -b_{h-1} \u0026 \\cdots \u0026 -b_2 \\\\ b_2 \u0026 b_1 \u0026 b_0 \u0026 \\cdots \u0026 -b_3 \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ b_{h-1} \u0026 b_{h-2} \u0026 b_{h-3} \u0026 \\cdots \u0026 b_0 \\end{bmatrix} $$ Security and estimator # To estimate the security of a ring SIS instance, we then effectively transform an RSIS instance $RSIS(h, w, q, \\beta, p)$ to an SIS instance $SIS(h, h \\cdot w, q, \\beta, p)$. You can instanciate an instance of RSIS in the estimator via RSIS.new(h, w, q, length_bound, Norm).\nModule SIS # Definition:[2]\nWe define the Module Short Integer Solution (Module-SIS) problem as follows $MSIS(h, w, d, q, \\beta, p)$. Let:\n\\( R = \\mathbb{Z}_q / \\langle x^h + 1 \\rangle \\), where \\( h \\) is a power of two, \\( \\mathcal{M} = R^d \\), a rank-\\( d \\) module over \\( R \\), Given $\\bold{a_1}, \\ldots, \\bold{a_w} \\in R_q^d$ chosen independently from the uniform distribution, find $s_1, \\ldots, s_w \\in R$ such that:\n$$ \\sum_{i=1}^{w}\\bold{a_i} s_i = 0 \\pmod{q} \\text{ and } 0 \u003c \\|\\mathbf{s}\\|_p \\leq \\beta $$where $\\bold{s} = (s_1, \\ldots, s_w)^T \\in R^m$.\nEach element $\\bold{a_i}$ can be seen as d coefficients in the ring and as such can be seen as an $h\\cdot d \\times h$ matrix. This leads a module SIS problem to be visualized as such in a standard SIS problem:\n\\[ \\mathbf{A} = \\begin{bmatrix} \\text{Rot}(a_{1,1}) \u0026 \\text{Rot}(a_{1,2}) \u0026 \\cdots \u0026 \\text{Rot}(a_{1,w}) \\\\ \\text{Rot}(a_{2,1}) \u0026 \\text{Rot}(a_{2,2}) \u0026 \\cdots \u0026 \\text{Rot}(a_{2,w}) \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\text{Rot}(a_{d,1}) \u0026 \\text{Rot}(a_{d,2}) \u0026 \\cdots \u0026 \\text{Rot}(a_{d,w}) \\end{bmatrix}, \\]where each block \\(\\text{Rot}(a_{i,j})\\) is a negacyclic matrix defined as:\n\\[ \\text{Rot}(b) = \\begin{bmatrix} b_0 \u0026 -b_{n-1} \u0026 -b_{n-2} \u0026 \\cdots \u0026 -b_1 \\\\ b_1 \u0026 b_0 \u0026 -b_{n-1} \u0026 \\cdots \u0026 -b_2 \\\\ b_2 \u0026 b_1 \u0026 b_0 \u0026 \\cdots \u0026 -b_3 \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ b_{n-1} \u0026 b_{n-2} \u0026 b_{n-3} \u0026 \\cdots \u0026 b_0 \\end{bmatrix}. \\] Security and estimator # To estimate the security of a module SIS instance, we then effectively transform an MSIS instance $MSIS(h, w, d, q, \\beta, p)$ to an SIS instance $SIS(h \\cdot d, h \\cdot w, q, \\beta, p)$. Also, note that by using $d=1$, we effectively come back to RSIS. Indeed, modules are a generalization of rings. You can instanciate an instance of MSIS in the estimator via MSIS.new(h, w, d, q, length_bound, Norm).\nReferences # J. Lambek,\u0026#32;Lectures on rings and modules.\u0026#32;American Mathematical Soc., 2009.\u0026#32;↩A. Langlois and\u0026#32;D. Stehlé,\u0026#32;Worst-case to average-case reductions for module lattices,\u0026#32;Designs, Codes and Cryptography,\u0026#32;vol. 75,\u0026#32;no. 3,\u0026#32;pp. 565–599,\u0026#32;2015.\u0026#32;↩ "},{"id":6,"href":"/lattirust_estimator.io/docs/sis-variants/","title":"SIS variants","section":"Docs","content":" SIS variants # We will now present all SIS problem variants implemented in our tools. Most effectively hand out additional hints to the adversary, while hoping that the problem remains hard. In all cases, we will try to define the SIS variants precisely, give the reduction to the basic SIS problem we use (SIS, RSIS, or MSIS) and also give a rough idea of what kind of scheme the problem variant allowed to construct. While we will try to be as close as possible from the original papers, this (and the entire webpage) could probably still contains mistakes. If you are involved in any of these schemes, feel free to reach out with any corrections. In all cases, we will also show an example of how the variants can be called via the estimator.\nk-SIS # k-SIS is the first variant of SIS that was introduced that handed out additional hints to the attacker. The key motivations behind k-SIS include:\nHomomorphic Signature Construction: k-SIS enables a secure way to produce signatures on linear combinations of authenticated vectors. In practice, this means that operations such as sums and weighted combinations of signed data (common in network coding) can be verified without re-signing the data. Removal of Random Oracles: The k-SIS assumption was used to eliminate reliance on the random oracle model for certain signature schemes. This provides a pathway to provably secure signature schemes in the standard model, albeit with constraints on the number of signatures. Efficient k-Time Signature Schemes: By using the k-SIS assumption, Boneh and Freeman created k-time signature schemes where the signing key is limited to signing at most k messages. This avoids the complexity of general existential unforgeability while still ensuring strong guarantees for a bounded number of operations. k-SIS definition [1]:\nAn instance of the k-SIS problem is represented by a matrix $\\bold{A}\\in \\mathbb{Z}_q^{h\\times w}$ and a set of $k$ vectors $\\bold{e_1},\\cdots, \\bold{e_k} \\in \\Lambda^T_q(\\bold{A})$ and the goal is to find another vector $\\bold{v}$ such that:\n$$ \\lVert \\bold{v} \\rVert \\leq \\beta \\text{, } \\bold{A}\\cdot\\bold{v} = \\bold{0} \\pmod q \\text{ and } \\bold{v} \\notin \\mathbb{Q}-span(\\lbrace \\bold{e_1},\\cdots, \\bold{e_k} \\rbrace) $$ Security and estimator # Theorem: Suppose $w\\geq 2h \\log q$, $\\frac{w}{k} \u0026gt; h$, $\\sigma \u0026gt; \\omega(\\sqrt{\\log w})$, $t\u0026gt; \\omega(\\sqrt{\\log h})$ and $q \u0026gt; \\sigma\\omega(\\sqrt{\\log w})$. If we let $\\beta\u0026rsquo; = \\beta(k^{\\frac{3}{2} + 1})k!(t\\sigma)^k$, then if we can solve the $k-SIS(h, w, q, \\beta, \\sigma, k, p)$ problem, then we can solve the $SIS(h, w-k, q, \\beta\u0026rsquo;, \\sigma, p)$ problem.\nYou can call an SIS instance in the estimator as kSIS.new(h, w, q, length_bound, sigma, k, Norm)\nk-M-SIS and k-R-SIS # k-M-SIS # k-M-SIS and k-R-SIS are the k-SIS versions lifted up in the module and rings settings respectively.\nk-M-SIS definition [2]:\nFor any integer $k\\geq 0$, an instance of the k-M-SIS problem is represented by a matrix $\\bold{A}\\in R_q^{h\\times w}$ and a set of k vectors $\\bold{v_0}, \\ldots, \\bold{v_{k-1}}$ such that:\n$$\\bold{A}\\cdot\\bold{v_i} = 0 \\pmod q \\text{ and } \\lVert \\bold{v_i} \\rVert \\leq \\beta$$and the goal is to find a non-vector $\\bold{v^*} \\in R^w$ such that:\n$$\\lVert \\bold{v^*} \\rVert \\leq \\beta^* \\text{ and }\\bold{v}\\notin \\mathcal{K}-span(\\lbrace \\bold{v_0}, \\ldots, \\bold{v_{k-1}}\\rbrace)$$ Security and estimator # No proof was provided in the original paper but following our previous work on reducing M-SIS to SIS and k-SIS to SIS we reduced an instance $k-M-SIS(h, w, d, q, \\beta, k, p)$ to an instance of $M-SIS(h, w - k, d, q, \\beta, k, p)$ that itself gets reduced to a standard SIS instance. You can instantiate a k-M-SIS instance instance via KMSIS.new(h, w, d, q, length_bound, k, Norm).\nk-R-SIS # Let\u0026rsquo;s simply note that k-R-SIS is k-M-SIS when $h=1$\nSecurity and estimator # Again we use the same reduction from $k-R-SIS(h, w, q, \\beta, k, p)$ to $R-SIS(h, w - k, q, \\beta, k, p)$ and then to standard SIS. You can instantiate an instance of k-R-SIS via\nKRSIS.new(h, w, q, length_bound, k, Norm).\nBASIS # The BASIS assumption was introduced to overcome limitations in existing lattice-based commitments, such as small message space and lack of succinctness for functional commitments. In particular, vector commitments and polynomial commitments require efficient mechanisms to open and verify data positions or evaluations without revealing the entire committed data. Previous approaches, like those based on collision-resistant hash functions (e.g., Merkle trees), or lattice-based schemes (e.g., SIS commitments), either required interaction or imposed stringent bounds on the data\u0026rsquo;s size. The BASIS assumption addresses these constraints by:\nSupporting Larger Message Spaces: Traditional SIS-based commitments require inputs to be \u0026ldquo;small\u0026rdquo; for correctness and security. BASIS allows commitments for inputs over the entire ring. Improving Private Openings: The construction based on BASISrand ensures that commitments and openings are statistically close to uniform, facilitating privacy-preserving operations. Enhancing Functional Openings: By incorporating structured randomness (as seen in BASISstruct), the framework supports opening commitments to computed values, such as the output of Boolean circuits or polynomial evaluations. BASIS$_{rand}$ # BASIS definition [3]:\nAn instance of the BASIS problem can be represented with a matrix $\\bold{A} \\in \\mathbb{Z}_q^{h\\times w}$, $s$ a Gaussian width parameter and a security parameter $\\lambda$. Let\u0026rsquo;s imagine a sampling algorithm Samp that takes $\\bold{A}$ as input and outputs a matrix $\\bold{B} \\in \\mathbb{Z}_q^{h\u0026rsquo;\\times w\u0026rsquo;}$ and an auxiliary input aux. We say that basis augmented SIS BASIS holds with respect to the sampling algorithm Samp if for all efficient adversary $\\mathcal{A}$, the following holds:\nBASISstruct.new(h, w, q, length_bound, sigma, k, Norm)\n$$ \\Pr\\left[ A \\mathbf{x} = 0 \\text{ and } 0 \u003c \\|\\mathbf{x}\\| \\leq \\beta : \\begin{array}{l} A \\leftarrow \\mathbb{Z}_q^{h \\times w}; \\\\ (\\mathbf{B}, \\text{aux}) \\leftarrow \\text{Samp}(1^\\lambda, A); \\\\ \\mathbf{T} \\leftarrow \\mathbf{B}_s^{-1}(G_{h'}); \\\\ \\mathbf{x} \\leftarrow \\mathcal{A}(1^\\lambda, A, \\mathbf{B}, \\mathbf{T}, \\text{aux}) \\end{array} \\right] = \\text{negl}(\\lambda). $$ Essentially, this SIS problem should remain hard even given a Trapdoor matrix $\\bold{B}$. This means that if $\\bold{B}$ contains too much information about $\\bold{A}$, the problem becomes feasible. $\\text{BASIS}_{rand}$ is a concrete instanciation that is defined as follows.\nThe sampling algorithm $\\text{Samp}(1^\\lambda, A)$ is defined as follows:\nSamples $i^* \\leftarrow [\\ell]$. Sets $A_i \\leftarrow \\mathbb{Z}_q^{(h+1) \\times w}$ for all $i \\neq i^*$, and $A \\leftarrow \\mathbb{Z}_q^w$. Defines $$ A_{i^*} = \\begin{bmatrix} \\mathbf{a^T} \\\\ \\bold{A} \\end{bmatrix}. $$ Outputs $$ \\mathbf{B}_\\ell = \\begin{bmatrix} A_1 \u0026 \u0026 \u0026-G_{\\eta+1} \\\\ \u0026 \\ddots \u0026 \u0026 \\vdots \\\\ \u0026 \u0026 A_\\ell \u0026 -G_{\\eta+1} \\end{bmatrix}, $$ and $\\text{aux} = i^*$. This is referred to as \u0026ldquo;the BASIS assumption with random matrices.\u0026rdquo;\nSecurity and estimator # Theorem: Let $\\lambda$ a security parameter and take any polynomial $\\ell$. If we suppose $h \\geq \\lambda$, $w\\geq O(h\\log q)$ and $s \\geq O(\\ell w\\log(h\\ell))$. Then under the $SIS(h,w,q,\\beta, p)$ assumption, the BASIS assumption holds with parameters $BASIS_{rand}(h,w,q,\\beta,s,\\ell, p)$. The reduction is straightforward as SIS needs to hold for $A_i$ even given $B_\\ell$.\nIn our tool, we therefore simply reduce $BASIS_{rand}(h,w,q,\\beta,s,\\ell, p)$ to $SIS(h,w,q,\\beta, p)$ you can call an instance of BASIS$_{rand}$ by instancianting a BASIS class as\nBASISrand.new(h, w, q, length_bound, sigma, k, Norm)\nwhere $k$ is $\\ell$, length_bound is $\\beta$, sigma is $s$ and Norm is either L2 or Linf.\nBASIS$_{struct}$ # Another instanciation is presented in the same paper as follows.\nThe sampling algorithm $\\text{Samp}(1^\\lambda, A)$ is defined as follows:\nSamples $W_i \\leftarrow \\mathbb{Z}_q^{h \\times h}$ for all $i \\in [\\ell]$. Outputs $$ \\mathbf{B}_\\ell = \\begin{bmatrix} W_1 A \u0026 \u0026 \u0026-G_{\\eta} \\\\ \u0026 \\ddots \u0026 \u0026 \\vdots \\\\ \u0026 \u0026 W{\\ell}A \u0026 -G_{\\eta} \\end{bmatrix}, $$ and $\\text{aux} = (W_1, \\ldots, W_\\ell)$. This is essentially $BASIS_{rand}$ with structured matrices $A_1, \\cdots, A_{l}$. It is referred to as \u0026ldquo;the BASIS assumption with structured matrices.\u0026rdquo;\nSecurity and estimator # While reducing to standard SIS is not possible for this structured version, the authors have shown that given an algorithm to solve $BASIS_{struct}$, we can solve k-M-ISIS as long as as the $BASIS_{struct}$ instance is of size $\\ell \u0026lt; \\frac{h}{k\u0026rsquo;}$ where k\u0026rsquo; is the parameter k in the k-M-ISIS instance. Our tool therefore reduces $BASIS_{struct}(h,w,q,\\beta,s,\\ell, p)$ to $k-M-ISIS(h,w,\\ell, q, \\beta, s, k\u0026rsquo;, p)$ You can instanciate an instance of $BASIS_{struct}$ via:\nBASISstruct.new(h, w, q, length_bound, sigma, k, Norm)\nPRISIS # PRISIS (Power-Ring Short Integer Solution) is a variant of the BASIS assumption designed to strengthen lattice-based cryptographic commitment schemes. Unlike its predecessors, PRISIS introduces a structure where matrix multipliers in the commitment scheme are replaced by scalar multiplications with random ring elements, specifically uniformly random polynomials from a defined ring $R_q$. The primary motivation for introducing PRISIS is to simplify the structure of the sampling process in lattice-based schemes while maintaining robust binding and security guarantees. PRISIS allows for more efficient commitment constructions by reducing the complexity of operations and maintaining small proof sizes. Furthermore, it retains security under standard lattice assumptions, supporting succinct, zero-knowledge proofs of polynomial evaluations, even for large degrees. This makes it particularly useful for applications requiring succinct non-interactive arguments and zero-knowledge proofs with minimal verification overhead.\nLet us first define PowerBASIS as a first step. We denote GL($n, R$) to be the group of $n\\times n$ invertible matrices over the ring R.\nThe sampling algorithm $\\text{Samp}(1^\\lambda, A)$ is defined as follows:\nGenerates a row $a^T\\leftarrow R^\\ell_q$. Sets \\[ A^* = \\begin{bmatrix} \\mathbf{a^\\top} \\\\ \\mathbf{A} \\end{bmatrix} \\in R_q^{(h+1) \\times w}. \\] Samples $\\bold{W} \\leftarrow \\text{GL}(h+1,R_q)$ Outputs $$ \\mathbf{B}_\\ell = \\begin{bmatrix} \\bold{W}^0\\bold{A^*} \u0026 \u0026 \u0026-G_{h+1} \\\\ \u0026 \\ddots \u0026 \u0026 \\vdots \\\\ \u0026 \u0026 \\bold{W}^\\ell\\bold{A^*} \u0026 -G_{h+1} \\end{bmatrix}, $$ and $\\text{aux} = \\bold{W}$. Let us now define the proper PRISIS assumption.\nPRISIS definition [4]:\nThe sampling algorithm $\\text{Samp}(1^\\lambda, A)$ is defined as follows:\nGenerates a row $a^T\\leftarrow R^\\ell_q$. Sets \\[ A^* = \\begin{bmatrix} \\mathbf{a^\\top} \\\\ \\mathbf{A} \\end{bmatrix} \\in R_q^{(h+1) \\times w}. \\] Samples $w \\leftarrow \\text{GL}(1,R_q)$ Outputs $$ \\mathbf{B}_\\ell = \\begin{bmatrix} w^0\\bold{A^*} \u0026 \u0026 \u0026-G_{h+1} \\\\ \u0026 \\ddots \u0026 \u0026 \\vdots \\\\ \u0026 \u0026 w^\\ell\\bold{A^*} \u0026 -G_{h+1} \\end{bmatrix}, $$ and $\\text{aux} = w$. Security and estimator # The security have only been analyzed for 2 hints, so $l=2$ but it has been shown that $BASIS_{struct}$ and $BASIS_{power}$ are equivalent in hardness for certain parameter choices. More directly for PRISIS we have that: Theorem: Let $h\u0026gt;0, w\\geq h$ and $t=(h+1)(\\lfloor \\log_{\\delta}(q)\\rfloor + 1)$. Let $q=\\omega(N)$, $\\epsilon \\in (0, \\frac{1}{3})$ and $s\\geq \\max(\\sqrt{N\\ln(8Nq)}q^{\\frac{1}{2} + \\epsilon}, \\omega(N^{\\frac{3}{2}}\\ln(N)^{\\frac{3}{2}}))$ such that $2^{10N}q^{-\\lfloor \\epsilon N \\rfloor}$ is negligible. Then for $\\sigma \\geq \\delta \\sqrt{tN(N^2s^2w + 2t)}\\omega(\\sqrt{N\\log h N})$, we have that PRISIS is hard under the $M-SIS(h, w, N, q, \\beta, p)$ assumption.\nWe therefore reduce a PRISIS instance to an M-SIS instance and therefore an M-SIS instance to a standard SIS instance. You can instanciate a PRISIS instance via\nPRISIS.new(h, w, d, q, length_bound, sigma, k, Norm).\nh-PRISIS # h-PRISIS definition [4]:\nh-PRISIS is a multi-instance PRISIS problem. The problem can be summarized as given $(\\lbrace \\bold{A_i} \\rBrace, \\lbrace \\bold{B_i} \\rBrace, \\lbrace w_i \\rBrace, \\lbrace \\bold{T_i} \\rBrace)_{i=0}^{h-1}$ defined as classical PRISIS problems, find short vectors $\\bold{x_i}$ s.t. $\\sum \\bold{A_i}\\cdot\\bold{x_i} = \\bold{0}$\nISISf # The ISISf assumption was proposed to overcome limitations in existing lattice-based constructions, particularly in creating privacy-preserving cryptographic schemes like blind signatures, group signatures, and anonymous credentials. The primary motivations include:\nEfficiency in Privacy-Preserving Credentials: traditional lattice-based anonymous credential schemes often required complex and inefficient proof systems to verify signatures. ISISf allows succinct zero-knowledge proofs for verifying structured relations. Generalized Pre-Image Relations: unlike the classic SIS problem, ISISf supports verifying that a short vector maps to an arbitrary function value $f(x)$ rather than simply being zero. This capability is essential for proving knowledge of secret values (e.g., credentials) in a way that hides sensitive information. Compatibility with Function Families: by introducing a function $f$, the assumption supports constructions where the output space is structured, such as binary encodings or commitments. ISISf notably enables:\nBlind Signatures : ISISf enables blind signature schemes where a signer issues a signature without learning the message content. The function f ensures that the relationship between the signature and the blinded message remains secure and unlinkable.\nAnonymous Credentials: Anonymous credentials allow users to prove ownership of credentials without revealing their full attribute set. The ISISf assumption allows efficient commitment to and selective disclosure of credential components. This makes it feasible to verify partial information while preserving privacy.\nGroup Signatures: Group signatures require a signer to remain anonymous within a group. ISISf supports efficient zero-knowledge proofs of group membership while ensuring that the signer cannot produce more valid signatures than their allowed number.\nZero-Knowledge Proofs of Knowledge (ZKPoK): The structure of ISISf supports constructing non-interactive zero-knowledge proofs for lattice relations. This leads to smaller and faster proofs compared to prior constructions, particularly when f is chosen as a linear or random function.\nISISf definition [5]: An instance of the ISISf problem is defined as follows:\nLet $\\boldsymbol{A} \\in \\mathbb{Z}_q^{h \\times w}$ and $f: \\mathbb{Z}_N \\rightarrow \\mathbb{Z}_q^h$ be a function. Given $(\\boldsymbol{A}, f)$ and access to an oracle that samples fresh $x \\leftarrow \\mathbb{Z}_N$ and outputs $x, \\boldsymbol{u}_x$ such that:\n$$ \\boldsymbol{A} \\cdot \\boldsymbol{u}_x = f(x) \\mod q \\quad \\text{and} \\quad \\lVert \\boldsymbol{u}_x \\rVert \\leq \\beta, $$where k is the number of times the oracle is used (number of hints).\nthe goal is to output a fresh tuple $(x^*, \\boldsymbol{u}^*)$ such that:\n$$ \\boldsymbol{A} \\cdot \\boldsymbol{u}^* = f(x^*) \\mod q \\quad \\text{and} \\quad \\lVert \\boldsymbol{u}^* \\rVert \\leq \\beta^*. $$ Security and estimator # The hardness of the ISISf assumption effectively depends on the function f chosen. If we consider f to be in the ROM (Random Oracle Model), then ISISf is as hard as SIS. However, if we want to attack the scheme without making any assumption about f, then we have to reduce to k-M-ISIS but the actual reduction would depend on the actual instanciation of the ISISf function and as such, we provide only the ROM road.\nIn the ROM case we reduce $ISISf(h,w,q,\\beta, k, p)$ to $SIS(h,w - k,q,\\beta, p)$ and you can call ISIf.new(h, w, q, length_bound, sigma, k, Norm).\nk-R-ISIS # The motivation for k-R-ISIS arises from the need to construct lattice-based succinct non-interactive arguments of knowledge (SNARKs) that:\nSupport recursive composition: recursive SNARKs require efficient verification of algebraic relations over bounded-norm vectors. Enable polynomial commitments beyond linear functions: many SNARK applications, especially in zero-knowledge systems, involve verifying polynomial maps with constant degrees greater than 1. Definitions [2]:\nk-M-ISIS admissibility:\nLet $g(\\bold{X})\\in R(\\bold{X})$ be a Laurent monomial (which is simply a monomial with possibly negative powers). We can write $g(\\bold{X}) = \\bold{X^e} = \\prod_{i\\in\\mathbb{Z_l}}\\bold{X}^{e_i}_i$ for some exponent vector $\\bold{e}\\in \\mathbb{Z}^l$. Now let $\\mathcal{G} \\subset R(\\bold{X})$ be a set of Laurent monomials and $|\\mathcal{G}|=k$ and $g^*\\in R(\\bold{X})$ be a target Laurent monomial.\nWe call a family $\\mathcal{G}$ k-M-ISIS admissible if\nAll $g\\in \\mathcal{G}$ have a constant degree All $g\\in \\mathcal{G}$ are distinct $0\\notin \\mathcal{G}$ and we call a family $(\\mathcal{G},g^*)$ k-M-ISIS admissible if $\\mathcal{G}$ is k-M-ISIS admissible and\n$g^*$ has constant degree $g^* \\notin \\mathcal{G}$ k-M-ISIS assumption: Now, let $\\mathbf{t} = (1, 0, \\ldots, 0)$. Let $\\mathcal{G} \\subseteq R(\\mathbf{X})$ be a set of $d$-variate Laurent monomials. Let $g^* \\in R(\\mathbf{X})$ be a target Laurent monomial. Let $(\\mathcal{G}, g^*)$ be $k$-M-ISIS-admissible. Let $\\mathbf{A} \\leftarrow R_q^{h \\times w}$, $\\mathbf{v} \\leftarrow (R_q^{*})^{d}$.\nThe K-M-ISIS assumption states that given $(\\mathbf{A}, \\mathbf{v}, \\mathbf{t}, {\\mathbf{u}_g})$ with $\\mathbf{u}_g$ short and\n\\[ g(\\mathbf{v}) \\cdot \\mathbf{t} \\equiv \\mathbf{A} \\cdot \\mathbf{u}_g \\mod q \\]it is hard to find a short $\\mathbf{u}_{g^{*}}$ and small $\\mathbf{s}^{*}$ such that:\n\\[ \\mathbf{s}^* \\cdot g^*(\\mathbf{v}) \\cdot \\mathbf{t} \\equiv \\mathbf{A} \\cdot \\mathbf{u}_{g^*} \\mod q. \\]When $h = 1$, i.e., when $\\bold{A}$ is just a vector, we call the problem $k$-R-ISIS.\nSecurity and estimator # The authors explore different special cases with parameters that do not really fit any real applications but show that the problem seems to be at least as hard as R-SIS. Several attacks can be considered, to evaluate the security of this scheme we rely upon the direct SIS attack on $\\bold{A}$, so we reduce $k-R-ISIS(h, w, q, \\beta, k, p)$ to $SIS(h, (w+1)h, q, \\beta, p)$. We similarly reduce $k-M-ISIS(h, w, d, q, \\beta, k, p)$ to $SIS(dh, d(w+1)h, q, \\beta, p)$. You can instanciate these SIS assumptions via\nKRISIS.new(h, w, q, length_bound, k, Norm).\nKMISIS.new(h, w, d, q, length_bound, k, Norm).\nvanishing-SIS # The Vanishing Short Integer Solution (vSIS) assumption introduces a novel approach to constructing lattice-based cryptographic schemes using vanishing polynomials, enabling succinct and homomorphic commitment schemes that are logarithmic in size relative to the input and support bounded multiplicative homomorphism. These commitments can be “folded,” compressing proof sizes in a manner similar to Bulletproofs. The vSIS assumption allows for efficient construction of succinct non-interactive arguments of knowledge (SNARKs) with quasi-linear prover time and polylogarithmic verifier runtime for structured relations, making verifiable delay functions (VDFs) possible in a lattice-based setting. Additionally, vSIS supports recursive proof composition and enhances prover efficiency, overcoming previous bottlenecks in lattice-based protocols. Building on the hardness of finding short vanishing polynomials at specific points, the vSIS problem extends the classical SIS problem, is no weaker than the k-R-ISIS assumption, and relates to the NTRU problem under specific conditions, offering robust post-quantum security guarantees.\nDefinition [6]:\nLet $h, w, d, q, \\beta \\in \\mathbb{N}$ and $\\mathcal{G}$ be a set of w-variate monomials of degree at most d. The vanishing SIS problem is represented by a set $V=\\lbrace \\bold{v_i} \\rbrace_{i=1}^n \\in (R^{\\times}_q)^w$ of $h$ uniformely random points in $(R^{\\times}_q)^w$. The goal is to find a non-zero polynomial $p\\in R[X_1, \\cdots, X_w]$ with monomial support over $\\mathcal{G}$ such that\n$$\\forall i \\in [n], p(\\bold{v_i}) = 0 \\pmod{q} \\text{ and } \\lVert p \\rVert \\leq \\beta$$ Security and estimator # The link between vanishing SIS and standard SIS can bee seen by writing the v-SIS problem as such\n$$ \\left( \\begin{array}{cccccccc} 1 \u0026 v_{1,1} \u0026 \\cdots \u0026 v_{1,w} \u0026 \\prod_{j=1}^{w} v_{1,j}^{e_{j}^1} \u0026 \\cdots \u0026 \\prod_{j=1}^{w} v_{1,j}^{e_{j}^d} \\\\ 1 \u0026 v_{2,1} \u0026 \\cdots \u0026 v_{2,w} \u0026 \\prod_{j=1}^{w} v_{2,j}^{e_{j}^1} \u0026 \\cdots \u0026 \\prod_{j=1}^{w} v_{2,j}^{e_{j}^d} \\\\ \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\\\ 1 \u0026 v_{n,1} \u0026 \\cdots \u0026 v_{n,w} \u0026 \\prod_{j=1}^{w} v_{n,j}^{e_{j}^1} \u0026 \\cdots \u0026 \\prod_{j=1}^{w} v_{n,j}^{e_{j}^d} \\\\ \\end{array} \\right) \\cdot \\mathbf{p} = 0 \\mod q \\text{ and} \\quad \\|\\mathbf{p}\\| \\leq \\beta, $$and you can instanciate a vanishing SIS problem via\nVSIS.new(h, w, d, q, length_bound, k, Norm).\none-more-ISIS # The one-more-ISIS (Inhomogeneous Short Integer Solution) assumption is a lattice-based analogue of the one-more-RSA assumption introduced to address the challenges of constructing practical and efficient post-quantum blind signatures. The one-more-ISIS assumption involves a game where an adversary is given a matrix \\( C \\) and can make a limited number of preimage queries for vectors \\( t \\) such that \\( C y = t \\) while ensuring \\( y \\) is short. The adversary wins if it can output one additional valid pair \\( (y, t) \\) beyond the number of queries it made. This assumption was introduced to support unforgeability in blind signature schemes by ensuring that even when multiple signatures are requested, forgery remains computationally infeasible.\nThe one-more-ISIS assumption allows for the construction of efficient, two-round, lattice-based blind signature schemes that are secure under post-quantum settings. It replaces the need for general-purpose zero-knowledge arguments with more efficient lattice-based non-interactive zero-knowledge proofs (NIZKs) for linear relations, significantly reducing signature sizes and computational costs. This development enables the creation of practical blind signature schemes with smaller signatures (around 45 KB) and low runtime for both the signer and the user, making them suitable for use in e-cash, e-voting, and other privacy-preserving applications.\nDefinition [7]\nAn instance of the one-more-ISIS problem is represented by a matrix $\\bold{A}\\in\\mathbb{Z}_q^{h\\times w}$. An adversary receives this matrix and is able to perform two times of queries:\nSyndrome queries: request a vector $\\bold{t}\\leftarrow \\mathbb{Z}_q^h$, the challenger keeps track of vectors sent by adding them to a set $\\mathcal{T}$.\nPreimage queries: submit a vector $\\bold{t\u0026rsquo;} \\in \\mathbb{Z}_q^h$ and the challenger gives back a short vector\n$$\\bold{y'} \\leftarrow D_{\\mathbb{Z^w,\\sigma}} \\text{ such that } \\bold{A}\\cdot \\bold{y'} = \\bold{t'} \\pmod q$$.\nWe denote the number of preimage queries done with $k$.\nThe adversary wins if he can output $k+1$ pairs $\\lbrace (\\bold{y_i}, \\bold{t_i}) \\rBrace_{0\\leq i \u0026lt; k+1}$ satisfying\n$$\\bold{A} \\cdot \\bold{y_i} = \\bold{t_i} \\pmod q \\text{, } \\lVert \\bold{y_i} \\rVert\\leq \\beta \\text{ and } \\bold{t_i}\\in\\mathcal{T}$$ Security and estimator # The security of the scheme is evaluated with respect to the following lattice reduction attack described in [7]:\nGiven $\\bold{A}$, compute a basis of $\\Lambda^T_q(A)$. Make $\\Theta(w^2)$ preimage queries for $\\bold{t} = 0$ and construct a new basis $\\bold{B}$ for $\\Lambda^T_q(C)$ such that the GSO $\\bold{\\overline{B}}$ of $\\bold{B}$ are bounded from above. Given an input $\\bold{t} \\in \\mathbb{Z}^h_q$ find any $\\bold{z}\\in \\mathbb{Z}$ such that $\\bold{C}\\bold{z} = \\bold{t}$ Run Babai\u0026rsquo;s nearest plane algorithm on $(\\bold{B}, \\bold{z})$. Let $\\bold{v}\\in \\Lambda^T_q(C)$ be the output tjem we return $\\bold{z}-\\bold{v}$ as solution for one-more-ISIS for $\\bold{t}$. As such, we reduce the security of the one-more-ISIS problem to solving SIS on a matrix about the size of $\\bold{A}$. You can instanciate a one-more SIS problem via onemoreISIS.new(h, w, q, length_bound, Norm).\nReferences # D. Boneh and\u0026#32;D. Freeman,\u0026#32;Linearly homomorphic signatures over binary fields and new tools for lattice-based signatures,\u0026#32;In Proc. International workshop on public key cryptography,\u0026#32;2011, pp. 1–16.\u0026#32;↩M. Albrecht,\u0026#32;V. Cini,\u0026#32;R. Lai,\u0026#32;G. Malavolta, and\u0026#32;S. Thyagarajan,\u0026#32;Lattice-based snarks: Publicly verifiable, preprocessing, and recursively composable,\u0026#32;In Proc. Annual international cryptology conference,\u0026#32;2022, pp. 102–132.\u0026#32;↩H. Wee and\u0026#32;D. Wu,\u0026#32;Succinct vector, polynomial, and functional commitments from lattices,\u0026#32;In Proc. Annual international conference on the theory and applications of cryptographic techniques,\u0026#32;2023, pp. 385–416.\u0026#32;↩G. Fenzi,\u0026#32;H. Moghaddas, and\u0026#32;N. Nguyen,\u0026#32;Lattice-based polynomial commitments: Towards asymptotic and concrete efficiency,\u0026#32;Journal of Cryptology,\u0026#32;vol. 37,\u0026#32;no. 3,\u0026#32;p. 31,\u0026#32;2024.\u0026#32;↩J. Bootle,\u0026#32;V. Lyubashevsky,\u0026#32;N. Nguyen, and\u0026#32;A. Sorniotti,\u0026#32;A framework for practical anonymous credentials from lattices,\u0026#32;In Proc. Annual international cryptology conference,\u0026#32;2023, pp. 384–417.\u0026#32;↩V. Cini,\u0026#32;R. Lai, and\u0026#32;G. Malavolta,\u0026#32;Lattice-based succinct arguments from vanishing polynomials,\u0026#32;In Proc. Annual international cryptology conference,\u0026#32;2023, pp. 72–105.\u0026#32;↩S. Agrawal,\u0026#32;E. Kirshanova,\u0026#32;D. Stehlé, and\u0026#32;A. Yadav,\u0026#32;Practical, round-optimal lattice-based blind signatures,\u0026#32;In Proc. Proceedings of the 2022 acm sigsac conference on computer and communications security,\u0026#32;2022, pp. 39–53.\u0026#32;↩ "},{"id":7,"href":"/lattirust_estimator.io/docs/estimator-options/","title":"Estimator options and API","section":"Docs","content":" Our tool # In this section, we will provide an overview of how to use the lattirust estimator. We will present the options for costing the problems, the type of errors you may encounter and also the search functions we provide over the parameter spaces and their characteristic.\nInstanciating and evaluating the hardness of SIS instances # You can set the type and the parameters of an SIS-based instance via:\nSIS.new(h, w, q, length_bound, Norm) for a classical SIS instance where $\\bold{A}\\in \\mathbb{Z}^{h\\times w}_q$ and you look for a vector such that $ \\boldsymbol{A} \\boldsymbol{y} = \\boldsymbol{0} \\mod q \\quad \\text{and} \\quad \\lVert\\boldsymbol{y}\\rVert \\leq$ length_bound. The norm is selected via a Rust enum as Norm::L2 or Norm::Linf. For example,\nlet falcon512_unf: SIS = SIS::new(512, 1024, 12289u64.into(), 5833.9072, Norm::L2);\nMSIS.new(h, w, d, q, length_bound, Norm) for a module SIS instance where the additional parameter $d$ is the rank of the module used. Again the norm is selected via a Rust enum as Norm::L2 or Norm::Linf. For example,\nlet msis_instance: MSIS = MSIS::new(24, 512, 4, 122898899u64.into(), 5833.9072, Norm::L2);\nFor specific variants that have extra parameters, please refer to the page presenting all variants to see how to instanciate them.\nNow to evaluate the hardness of an instance via a lattice reduction attack, you must choose the specific model you want to use for the SVP oracle that is part of BKZ and the simulator you want to use for the assumed shape of vectors after lattice reduction. If you evaluate the hardness of an SIS instance under an euclidean norm, then the actual shape assumed after lattice reduction does not matter as we directly optimize over a function between the hermite factor and the block-size, however, it does matter under an infinity norm. You get the hardness by calling\nsis_instance.security_level(estimate_type, simulator_type)-\u0026gt;Result\u0026lt;f64, LatticeEstimatorError\u0026gt; estimate_type is an Option\u0026lt;Estimates\u0026gt; where Estimates is a Rust enum, where None defaults to Matzov(true) simulator_type is an Option\u0026lt;Simulator\u0026gt; where Simulator is a Rust enum, where None defaults to GSA The return value of the security_level is either a lattice estimator error, or a float $\\lambda$ that represents the security level $2^\\lambda$ The Estimates enum # pub enum Estimates { BdglSieve, // Implements the Becker-Ducas-Gama-Laarhoven sieve QSieve, // Quantum sieve model for quantum-enhanced sieving BjgSieve, // Bai-Jeng-Groves sieve for lattice sieving optimizations AdpsSieve(bool), // ADPS sieve (Albrecht-Ducas-Poppelmann-Steinwandt), with optimizations for dual attack settings ChaLoySieve, // Chatterjee-Loyd sieve for structured lattices CheNgueEnum, // Chen-Nguyen enumeration model combining enumeration with sieving AbfEnum(bool), // ABF enumeration technique with an optimization flag AblrEnum, // Albrecht-Bai-Laarhoven-Reith (ABLR) enumeration LotusEnum, // LOTUS enumeration for module-SIS problems Kyber(bool), // Kyber-specific estimator with an optimization boolean Matzov(bool) // Matzov estimator with optional advanced heuristics } For Estimates that have a bool integrated to them, it is because they offer both a classical and a quantum estimate. So you if set Matzov(true) you will get the classical estimates whereas is you set Matzov(false) you will get the quantum estimate,\nThe Simulator enum # pub enum Simulator { GSA, // Geometric Series Assumption (exponential decay in norms) ZGSA, // Z-shaped profile for q-ary lattices with plateaus LGSA // L-shaped profile for q-vectors that are rerandomized } Lattice estimator errors # While using our tool, you may encounter three types of error all represented by an enum.\npub enum LatticeEstimatorError { /// Invalid parameter error with details InvalidParameter { param_name: String, reason: String, }, /// Computation-related error ComputationError { message: String, }, /// Configuration error with a specific message ConfigurationError { message: String, } } They can be thrown either while actually costing a scheme or at the beginning because of parameters that can not be used. They represent critical errors and the estimator can not run while encountering one of these. You may also encounter another type or \u0026ldquo;errors\u0026rdquo; that we call warnings. Warning are just possible problem that could present with the set of parameters you choose for a specific instance. Since they are mostly asymptotics, warnings have the goal to make you think about what parameters you are choosing because the reduction that is proven does not cover them or they seem sensitive given the asymptotic bound. These warnings will print on the standard output but the estimator will still be able to run. We did not want to stop the user everytime the parameter choices seemed to conflict with asymptotics bound to prevent too much restriction on usage.\nSearching for the best parameters on single SIS instances # For an already defined SIS instance, we provide a way to find the tightest set of parameters to achieve a certain level security. We give a way to optimize separately for $h$ which is the primary security indicator, but also for the length bound and for $d$ the dimension of the module and $k$ the number of hints, when it applies. We also give a function to optimize both principal parameters ($h$ and the length bound) simultaneously.\nSearching for the best parameters on multiple SIS instances # "}]